{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bike Rental Datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import libararies \n",
    "import pandas as pd\n",
    "import datetime as dt \n",
    "import zoneinfo\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_bikes = pd.read_csv(\"data/bike_rentals.csv\")\n",
    "df_bikes.head()\n",
    "df_bikes.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_bikes.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# finding number of null values \n",
    "df_bikes.isna().sum() # return cols name with null values \n",
    "df_bikes.isna().sum().sum()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# displaying null values \n",
    "df_bikes[df_bikes.isna().any(axis=1)] # .df_bikes.isna().any gathers any and all null values while (axis=1) specifies values in the columns."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_bikes['windspeed'].fillna(df_bikes['windspeed'].median(), inplace=True)\n",
    "# filling missing data with meidan is often better choice compared to mean, bcos median guarantees that half the\n",
    "# data is greater than the given value and half that is lower. Mean on the other other hand is vulnerable to\n",
    "# outliers\n",
    "\n",
    "df_bikes[df_bikes.isna().any(axis=1)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# group df by seanson with the median aggregate \n",
    "df_bikes.groupby(['season']).median()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# replaceing missing values in 'hum' column \n",
    "df_bikes['hum'] = df_bikes['hum'].fillna(df_bikes.groupby(['season'])['hum'].transform('median'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_bikes.iloc[[129, 213, 388]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# finding null values in temp. \n",
    "df_bikes[df_bikes['temp'].isna()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# getting mean of temp at instance 700 and 702\n",
    "mean_temp = (df_bikes.iloc[702]['temp'] + df_bikes.iloc[702]['temp']) / 2\n",
    "mean_atemp = (df_bikes.iloc[702]['atemp'] + df_bikes.iloc[702]['atemp']) / 2\n",
    "\n",
    "# replace missing temp and atemp values \n",
    "df_bikes['temp'].fillna(mean_temp, inplace=True)\n",
    "df_bikes['atemp'].fillna(mean_atemp, inplace=True)\n",
    "\n",
    "df_bikes.iloc[[701]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# converting 'dteday' to a datetime type \n",
    "df_bikes['dteday'] = pd.to_datetime(df_bikes['dteday'], infer_datetime_format=True)\n",
    "\n",
    "# replacing in yr\n",
    "df_bikes.loc[730, 'yr'] = 1.0 "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now extrapolate dates for the null values using some different approaches. A standard approach is convert the 'mnth' column to the correct months extrapolated from the 'dteday' column. This has the advantage of correcting any additional errors that may have surfaced in conversions, assuming of course that the 'dteday' column is correct.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_bikes['mnth'] = df_bikes['dteday'].dt.month\n",
    "df_bikes.tail()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# droping non numerial objects\n",
    "main_cols_save = df_bikes.columns.difference(['dteday','casual','registered'])\n",
    "df_bikes[main_cols_save].to_csv('data/df_bikes_cleaned.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_cols = df_bikes.columns.difference(['dteday','casual','registered', 'cnt'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = df_bikes[main_cols]\n",
    "y = df_bikes['cnt']\n",
    "X, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# splitting datasets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "l_reg = LinearRegression()\n",
    "\n",
    "l_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = l_reg.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"RMSE: %0.2f\" %(rmse))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_bikes['cnt'].describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xg_reg = XGBRegressor()\n",
    "xg_reg.fit(X_train, y_train)\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"RMSE: %0.2f\" %(rmse))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cross validation with linear reg \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "rmse = np.sqrt(-scores)\n",
    "\n",
    "print('Reg rmse:', np.round(rmse, 2))\n",
    "\n",
    "print('RMSE mean: %0.2f' % (rmse.mean()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Census"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_census = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', header=None)\n",
    "\n",
    "df_census.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_census.columns=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "df_census.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_census.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_cols = df_census._get_numeric_data().columns.tolist()\n",
    "cat_cols = [col for col in df_census.columns if col not in num_cols + ['education']]\n",
    "\n",
    "num_cols\n",
    "cat_cols"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_census = df_census.drop(['education'], axis=1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_census = pd.get_dummies(data = df_census, columns = cat_cols)\n",
    "df_census.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_cols_save = df_census.columns.difference(['education','income_ <=50K'])\n",
    "df_census[main_cols_save].to_csv('data/df_census_cleaned.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_cols = df_census.columns.difference(['education','income_ <=50K', 'income_ >50K'])\n",
    "main_cols"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = df_census[main_cols]\n",
    "y = df_census['income_ >50K']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def cross_val(classifier, num_splits=10):\n",
    "    model = classifier\n",
    "    scores = cross_val_score(model, X, y, cv=num_splits)\n",
    "    print('Accuracy:', np.round(scores, 2))\n",
    "    print('Accuarcy mean: %0.2f' %(scores.mean()))\n",
    "\n",
    "cross_val(LogisticRegression())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "cross_val(XGBClassifier(n_estimators=5))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_bikes = pd.read_csv('data/df_bikes_cleaned.csv')\n",
    "main_cols = df_bikes.columns.difference(['dteday','casual','registered', 'cnt'])\n",
    "X_bikes = df_bikes[main_cols]\n",
    "y_bikes = df_bikes['cnt']\n",
    "\n",
    "reg = DecisionTreeRegressor(random_state=2)\n",
    "scores = cross_val_score(reg, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=5)\n",
    "rmse = np.sqrt(-scores)\n",
    "\n",
    "print('RMSE mean: %0.2f' % (rmse.mean()))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_train)\n",
    "\n",
    "reg_mse = mean_squared_error(y_train, y_pred)\n",
    "reg_rmse = np.sqrt(reg_mse)\n",
    "reg_rmse"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "params = {'max_depth':[None,2,3,4,6,8,10,20]}\n",
    "\n",
    "reg = DecisionTreeRegressor(random_state=2)\n",
    "\n",
    "grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=5, n_jobs=2)\n",
    "\n",
    "grid_reg.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_reg.best_params_\n",
    "\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "best_score = np.sqrt(-grid_reg.best_score_)\n",
    "print(\"Training score: {:.3f}\".format(best_score))\n",
    "\n",
    "best_model = grid_reg.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print('Test score: {:.3f}'.format(rmse_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def grid_search(params, reg=DecisionTreeRegressor(random_state=2)):\n",
    "\n",
    "    grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "\n",
    "    grid_reg.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_reg.best_params_\n",
    "    print(\"Best params:\", best_params)\n",
    "    best_score = np.sqrt(-grid_reg.best_score_)\n",
    "    print(\"Training score: {:.3f}\".format(best_score))\n",
    "\n",
    "    y_pred = grid_reg.predict(X_test)\n",
    "    rmse_test = mean_squared_error(y_test, y_pred)**0.5\n",
    "\n",
    "    print('Test score: {:.3f}'.format(rmse_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search(params={'min_samples_leaf':[1, 2, 4, 6, 8, 10, 20, 30]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search(params={'max_depth':[None,2,3,4,6,8,10,20],'min_samples_leaf':[1,2,4,6,8,10,20,30]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search(params={'max_depth':[6,7,8,9,10],'min_samples_leaf':[3,5,7,9]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Heart disease dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_heart = pd.read_csv('data/heart_disease.csv')\n",
    "df_heart.head()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_cols = df_heart.columns.difference(['target'])\n",
    "X = df_heart[main_cols]\n",
    "y = df_heart['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=2)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print('Accuracy:', np.round(scores, 2))\n",
    "print('Accuracy mean: %0.2f' % (scores.mean()))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RandomizedSearch CLF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def randomized_search_clf(params, runs=20, clf=DecisionTreeClassifier(random_state=2)):\n",
    "    rand_clf = RandomizedSearchCV(clf, params, n_iter=runs, cv=5, n_jobs=3, random_state=2)\n",
    "    rand_clf.fit(X_train, y_train)\n",
    "    best_model = rand_clf.best_estimator_\n",
    "    best_score = rand_clf.best_score_\n",
    "    print(\"Training score: {:.3f}\".format(best_score))\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Test score: {:.3f}'.format(accuracy))\n",
    "\n",
    "    return best_model\n",
    "\n",
    "params = {'criterion':['entropy', 'gini'], 'splitter': ['random', 'best'],\n",
    "          'min_weight_fraction_leaf': [0.0, 0.0025, 0.005, 0.0075, 0.01],\n",
    "          'min_samples_split':[2, 3, 4, 5, 6, 8, 10], 'min_samples_leaf':[1, 0.01, 0.02, 0.03, 0.04],\n",
    "          'min_impurity_decrease':[0.0, 0.0005, 0.005, 0.05, 0.10, 0.15, 0.2],\n",
    "          'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None],\n",
    "          'max_features':['auto', 0.95, 0.90, 0.85, 0.80, 0.75, 0.70],\n",
    "          'max_depth':[None, 2,4,6,8],\n",
    "          'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05]}    \n",
    "\n",
    "randomized_search_clf(params=params);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = {'max_depth':[None, 6, 7],'max_features':['auto', 0.78],\n",
    "          'max_leaf_nodes':[45, None], 'min_samples_leaf':[1, 0.035, 0.04, 0.045, 0.05],\n",
    "          'min_samples_split':[2, 9, 10],'min_weight_fraction_leaf': [0.0, 0.05, 0.06, 0.07],}\n",
    "model_ = randomized_search_clf(params= params, runs=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
    "                               max_features=0.78, max_leaf_nodes=45, min_impurity_decrease=0.0,\n",
    "                               min_impurity_split=None, min_samples_leaf=0.045, min_samples_split=9,\n",
    "                               min_weight_fraction_leaf=0.06, random_state=2, splitter='best')\n",
    "\n",
    "scores = cross_val_score(model_, X, y, cv=10)\n",
    "\n",
    "print('Accuracy:', np.round(scores, 2))\n",
    "\n",
    "print('Accuracy mean: %0.2f' % (scores.mean()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature importance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_clf = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=9,max_features=0.8,\n",
    "                                  max_leaf_nodes=47,min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                  min_samples_leaf=1, min_samples_split=8,min_weight_fraction_leaf=0.05,\n",
    "                                  random_state=2, splitter='best')\n",
    "\n",
    "best_clf.fit(X, y)\n",
    "\n",
    "best_clf.feature_importances_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import operator\n",
    "\n",
    "feature_dict = dict(zip(X.columns, best_clf.feature_importances_))\n",
    "\n",
    "sorted(feature_dict.items(), key=operator.itemgetter(1), reverse=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bagging with Random Forests\n",
    "\n",
    "### Bootstrap aggreagation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_census = pd.read_csv('data/census_cleaned.csv')\n",
    "\n",
    "df_census.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_cols_census = df_census.columns.difference(['income_ >50K'])\n",
    "X_census = df_census[main_cols_census]\n",
    "y_census = df_census['income_ >50K']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=2, n_jobs=-1)\n",
    "scores = cross_val_score(rf, X_census, y_census, cv=5)\n",
    "\n",
    "print('Accuracy:', np.round(scores, 3))\n",
    "print('Accuracy mean: %0.3f' % (scores.mean()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_bikes.head()\n",
    "\n",
    "main_cols_bikes = df_bikes.columns.difference(['dteday','casual','registered', 'cnt'])\n",
    "X_bikes = df_bikes[main_cols_bikes]\n",
    "y_bikes = df_bikes['cnt']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=10, random_state=2, n_jobs=-1)\n",
    "scores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "rmse = np.sqrt(-scores)\n",
    "\n",
    "print('RMSE:', np.round(rmse, 3))\n",
    "print('RMSE mean: %0.3f' % (rmse.mean()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### oob_Score \n",
    "rf = RandomForestClassifier(oob_score=True, n_estimators=10, random_state=2, n_jobs=-1)\n",
    "rf.fit(X_census, y_census)\n",
    "\n",
    "rf.oob_score_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rf = RandomForestClassifier(oob_score=True, n_estimators=50, random_state=2, n_jobs=-1)\n",
    "rf.fit(X_census, y_census)\n",
    "\n",
    "rf.oob_score_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rf = RandomForestClassifier(oob_score=True, n_estimators=100, random_state=2, n_jobs=-1)\n",
    "rf.fit(X_census, y_census)\n",
    "\n",
    "rf.oob_score_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "oob_scores = []\n",
    "rf = RandomForestClassifier(n_estimators=50, warm_start=True, oob_score=True, n_jobs=-1, random_state=2)\n",
    "rf.fit(X_census, y_census)\n",
    "oob_scores.append(rf.oob_score_)\n",
    "\n",
    "est = 50\n",
    "estimators = [est]\n",
    "for i in range(9):\n",
    "    est += 50\n",
    "    estimators.append(est)\n",
    "    rf.set_params(n_estimators=est)\n",
    "    rf.fit(X_census, y_census)\n",
    "    oob_scores.append(rf.oob_score_)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(estimators, oob_scores)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('oob_score_')\n",
    "plt.title('Random Forest Warm Start', fontsize=15)\n",
    "plt.savefig('Random_Forest_Warm_Start', dpi=325)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rf = RandomForestRegressor(n_estimators=50, warm_start=True, n_jobs=-1, random_state=2)\n",
    "scores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)\n",
    "rmse = np.sqrt(-scores)\n",
    "\n",
    "print('RMSE:', np.round(rmse, 3))\n",
    "print('RMSE mean: %0.3f' % (rmse.mean()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fine-tunning hyperparameters\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)\n",
    "\n",
    "def randomized_search_Reg(params, runs=16, reg=RandomForestRegressor(random_state=2, n_jobs=-1)):\n",
    "    rand_reg = RandomizedSearchCV(reg, params, n_iter=runs, scoring='neg_mean_squared_error',\n",
    "                                  cv=10, n_jobs=-1, random_state=2)\n",
    "    rand_reg.fit(X_train, y_train)\n",
    "    best_model = rand_reg.best_estimator_\n",
    "    best_params = rand_reg.best_params_\n",
    "    best_score = np.sqrt(-rand_reg.best_score_)\n",
    "    print(\"Best params:\", best_params)\n",
    "    print(\"Training score: {:.3f}\".format(best_score))\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    rmse_test = mean_squared_error(y_test, y_pred)**0.5\n",
    "    print('Test set score: {:.3f}'.format(rmse_test))\n",
    "\n",
    "    return best_model\n",
    "    \n",
    "    \n",
    "params={'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05],\n",
    "        'min_samples_split':[2, 0.01, 0.02, 0.03, 0.04, 0.06, 0.08, 0.1],\n",
    "        'min_samples_leaf':[1,2,4,6,8,10,20,30],\n",
    "        'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],\n",
    "        'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None],\n",
    "        'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],\n",
    "        'max_depth':[None,2,4,6,8,10,20]}\n",
    "\n",
    "randomized_search_Reg(params=params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params={'min_samples_leaf': [1,2,4,6,8,10,20,30],\n",
    "        'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],\n",
    "        'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],\n",
    "        'max_depth':[None,2,4,6,8,10,20]}\n",
    "randomized_search_Reg(params=params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params={'min_samples_leaf':[1,2,4,6,8,10,20,30],\n",
    "        'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],\n",
    "        'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],\n",
    "        'max_depth':[None,4,6,8,10,12,15,20]}\n",
    "randomized_search_Reg(params=params, runs=20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params={'min_samples_leaf':[1,2,3,4,5,6],\n",
    "        'min_impurity_decrease':[0.0, 0.01, 0.05, 0.08, 0.10, 0.12, 0.15],\n",
    "        'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],\n",
    "        'max_depth':[None,8,10,12,14,16,18,20]}\n",
    "\n",
    "randomized_search_Reg(params=params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params={'min_samples_leaf':[1,2,4,6,8,10,20,30],\n",
    "        'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],\n",
    "        'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],\n",
    "        'max_depth':[None,4,6,8,10,12,15,20],'n_estimators':[100]}\n",
    "randomized_search_Reg(params=params, runs=20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rf = RandomForestRegressor(n_estimators=100,\n",
    "                           min_impurity_decrease=0.1,\n",
    "                           max_features=0.6, max_depth=12,\n",
    "                           warm_start=True, n_jobs=-1,\n",
    "                           random_state=2)\n",
    "\n",
    "scores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "rmse = np.sqrt(-scores)\n",
    "\n",
    "print('RMSE:', np.round(rmse, 3))\n",
    "\n",
    "print('RMSE mean: %0.3f' % (rmse.mean()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# shuffling data \n",
    "from sklearn.utils import shuffle\n",
    "df_shuffle_bikes = shuffle(df_bikes, random_state=2)\n",
    "\n",
    "X_shuffle_bikes = df_shuffle_bikes[main_cols_bikes]\n",
    "y_shuffle_bikes = df_shuffle_bikes['cnt']\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100,\n",
    "                           min_impurity_decrease=0.1,\n",
    "                           max_features=0.6, max_depth=12,\n",
    "                           warm_start=True, n_jobs=-1,\n",
    "                           random_state=2)\n",
    "\n",
    "scores = cross_val_score(rf, X_shuffle_bikes, y_shuffle_bikes, scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "rmse = np.sqrt(-scores)\n",
    "\n",
    "print('RMSE:', np.round(rmse, 3))\n",
    "\n",
    "print('RMSE mean: %0.3f' % (rmse.mean()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## From Gradient Boosting to XGBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Buiding Gradient Boost From Scratch "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)\n",
    "\n",
    "\n",
    "tree_1 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "tree_1.fit(X_train, y_train)\n",
    "y_train_pred = tree_1.predict(X_train)  # predict with trainset instead of testset.\n",
    "y2_train = y_train - y_train_pred # get residual \n",
    "\n",
    "tree_2 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "tree_2.fit(X_train, y2_train) # use residual as target for next training\n",
    "y2_train_pred = tree_2.predict(X_train)\n",
    "y3_train = y2_train -y2_train_pred\n",
    "\n",
    "tree_3 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "tree_3.fit(X_train, y3_train) # repeat as done earlier \n",
    "\n",
    "y1_pred = tree_1.predict(X_test) # predit test variable \n",
    "y2_pred = tree_2.predict(X_test)\n",
    "y3_pred = tree_3.predict(X_test)\n",
    "\n",
    "y_pred = y1_pred + y2_pred + y3_pred # summing up predictions \n",
    "mean_squared_error(y_test, y_pred)**0.5\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient boosting model using sklearn"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=3, random_state=2,\n",
    "                                learning_rate=1.0)\n",
    "\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)**0.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=30, random_state=2,\n",
    "                                learning_rate=1.0)\n",
    "\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)**0.5;"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2)\n",
    "\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)**0.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "learning_rate_values = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 1.0]\n",
    "\n",
    "for value in learning_rate_values:\n",
    "    gbr = GradientBoostingRegressor(max_depth=2,   n_estimators=300,\n",
    "                                    random_state=2, learning_rate=value)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred)**0.5\n",
    "    print('Learning Rate:', value, ', Score:', rmse)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Base Learner \n",
    "depths = [None, 1, 2, 3, 4]\n",
    "for depth in depths:\n",
    "    gbr = GradientBoostingRegressor(max_depth=depth,   n_estimators=300,\n",
    "                                    random_state=2)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred)**0.5\n",
    "    print(\"Max Depth:\", depth, \", Score:\", rmse);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# subsample \n",
    "samples = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.2]\n",
    "for sample in samples:\n",
    "    gbr = GradientBoostingRegressor(max_depth=3,   n_estimators=300, subsample=sample, random_state=2)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred)**0.5\n",
    "    print(\"Sample:\", sample, \", Score:\", rmse);\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = {'subsample': [0.5, 1.0, 0.0, 0.65, 0.70, 0.75, 0.02, 0.01],\n",
    "          'n_estimators': [300, 500, 1000, 1500, 1800, 2000],\n",
    "          'learning_rate': [0.05, 0.075, 0.1]}\n",
    "\n",
    "gbr = GradientBoostingRegressor(max_depth=3, random_state=2)\n",
    "\n",
    "rand_Reg = RandomizedSearchCV(gbr, params, n_iter=10,\n",
    "                              scoring='neg_mean_squared_error', cv=5,\n",
    "                              n_jobs=-1, random_state=2)\n",
    "rand_Reg.fit(X_train, y_train)\n",
    "best_model = rand_Reg.best_estimator_\n",
    "best_params = rand_Reg.best_params_\n",
    "best_score = np.sqrt(-rand_Reg.best_score_)\n",
    "\n",
    "print(\"Best params:\", best_params)\n",
    "print(\"Training score: {:.3f}\".format(best_score))\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "print('Test set score: {:.3f}'.format(rmse_test))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=3, n_estimators=1600, subsample=0.75, learning_rate=0.02, random_state=2)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)**0.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Xgboost\n",
    "xg_reg = XGBRegressor(max_depth=3, n_estimators=1600, eta=0.02, subsample=0.75, random_state=2)\n",
    "xg_reg.fit(X_train, y_train)\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)**0.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Big Data; XGBOOST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('data/exoplanets.csv')\n",
    "df.head();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.info()\n",
    "\n",
    "# checking for null values \n",
    "df.isnull().sum().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_cols = df.columns.difference(['LABEL'])\n",
    "\n",
    "X = df[main_cols]\n",
    "y = df['LABEL']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.countplot(y)\n",
    "plt.title('Target Distribution', fontdict={'size':14})\n",
    "\n",
    "y.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# grad boost classifer \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%timeit\n",
    "df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "gbr = GradientBoostingClassifier(n_estimators=100, max_depth=2, random_state=2)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print('Score: ' + str(score))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost Unveiling "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Building XGBoost Models\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# loading Iris dataset \n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['target'])  # np.c_ is concatenating two numpy arrays \n",
    "df.head()\n",
    "\n",
    "\n",
    "# preparing datasets for ml. \n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], random_state=2)\n",
    "\n",
    "xgb = XGBClassifier(booster='gbtree', objective='multi:softprob',\n",
    "                    max_depth=6, learning_rate=0.1,\n",
    "                    n_estimators=100, random_state=2,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "accuracy_score(y_pred, y_test)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Diabetes dataset \n",
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "xgb = XGBRegressor(booster=\"gbtree\", objective=\"reg:squarederror\",\n",
    "                   max_depth=6, learning_rate=0.1,\n",
    "                   n_estimators=100, random_state=2,\n",
    "                   n_jobs=-1)\n",
    "\n",
    "scores = cross_val_score(xgb, X, y, scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "rmse = np.sqrt(-scores)\n",
    "print('RMSE:', np.round(rmse, 3))\n",
    "print('RMSE mean: %0.3f' % (rmse.mean()))\n",
    "\n",
    "pd.DataFrame(y).describe()\n",
    "# xgboost regressor template (cross-validation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Higgs Challenge"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('data/atlas-higgs-challenge-2014-v2.csv.gz', nrows=250000,\n",
    "                 compression='gzip')\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "del df['Weight']\n",
    "del df['KaggleSet']\n",
    "\n",
    "df = df.rename(columns={'KaggleWeight': 'Weight'})\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "label_col = df['Label']\n",
    "del df['Label']\n",
    "df['Label'] = label_col\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# replacing target variable with 0 and 1\n",
    "df['Label'].replace(('s','b'), (1, 0), inplace=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_cols = df.columns.difference(['EventId', 'Label', 'Weight'])\n",
    "target_cols = ['Label']\n",
    "\n",
    "X = df[main_cols].squeeze()\n",
    "y = df[target_cols].squeeze();\n",
    "\n",
    "# X = df.iloc[:,1:31]\n",
    "# y = df.iloc[:,-1];"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "type(df.iloc[:,1:31]), type( df[main_cols].squeeze())\n",
    "type(df[target_cols].squeeze()), type(df.iloc[:,-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The weights should first be scaled to match the test data since the test data provides the expected number of signal \n",
    "# and background events generated by the test set. The test data has 550,000 rows, more than twice the 250,000 rows (len(y))\n",
    "# provided by the training data. Scaling weights to match the test data can be achieved by multiplying the weight column by\n",
    "# the percentage of increase, as follows:\n",
    "\n",
    "df['test_Weight'] = df['Weight'] * 550000 / len(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, XGBoost provides a hyperparameter, scale_pos_weight, which takes the scaling factor into account. The scaling factor is the sum of the weights of the background noises divided by the sum of the weight of the signal. The scaling factor can be computed using pandas conditional notation, as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s = np.sum(df[df['Label'] == 1]['test_Weight'])\n",
    "b = np.sum(df[df['Label'] == 0]['test_Weight']);\n",
    "\n",
    "scale_pos_weight = b/s\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from typing import Tuple\n",
    "import xgboost as xgb\n",
    "\n",
    "def f1_eval(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[str, float]:\n",
    "    y = dtrain.get_label()\n",
    "\n",
    "    # convert the predicted values from {predt E R | 0<predt<1} to {0, 1} with a threshold of 0.5\n",
    "    # all values less than 0.5 would be converted to 0 (False) and\n",
    "    # all values equal or greater than 0.5 would be converted to 1 (True)\n",
    "    predt_binary = np.where(predt > 0.5, 1, 0)\n",
    "    return \"F1_score\", f1_score(y_true=y, y_pred=predt_binary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_clf = xgb.DMatrix(X, y, missing=-999.0, weight=df['test_Weight'])\n",
    "\n",
    "param = {'objective':'binary:logitraw',\n",
    "         'scale_pos_weight': b/s,\n",
    "         'eta': 0.1,\n",
    "         'max_depth': 6,\n",
    "         'eval_metric': 'auc'}\n",
    "\n",
    "# list of params that include the preceeding items, along with the evaluation metric (auc) and ams@0.15\n",
    "plst = list(param.items()) + [('eval_metric', 'ams@0.15')]\n",
    "# create watchlist that include the intializaition  classifier and 'train' so that you can veiw scores as tree \n",
    "# cont. to boost \n",
    "watchlist = [(xgb_clf, 'train')]\n",
    "\n",
    "# num of boosting rounds \n",
    "num_round = 120\n",
    "\n",
    "print('loading data end, start to boost tree')\n",
    "bst = xgb.train(plst, xgb_clf, num_round, watchlist, feval=f1_eval)\n",
    "\n",
    "# save model \n",
    "bst.save_model('higgs.model')\n",
    "print('finish training');\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoody Hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# heart disease dataset\n",
    "df = pd.read_csv('data/heart_disease.csv')\n",
    "df.head()\n",
    "df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_cols_heart = df.columns.difference(['target'])\n",
    "X = df[main_cols_heart].squeeze()\n",
    "y = df['target'].squeeze()\n",
    "\n",
    "model = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=2, use_label_encoder =False)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print('Accuracy:', np.round(scores, 2))\n",
    "print('Accuracy mean: %0.2f' % (scores.mean()))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### StratifiedKfold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "\n",
    "# baseline model \n",
    "scores = cross_val_score(model, X, y, cv=kfold)\n",
    "print('Accuracy:', np.round(scores, 2))\n",
    "print('Accuracy mean: %0.2f' % (scores.mean()))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# combining gridsearch cv and randomizedsearch cv \n",
    "\n",
    "def grid_search(params: dict, random: bool =False) -> dict:\n",
    "    xgb = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=2, use_label_encoder =False)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2) \n",
    "    if random:\n",
    "        grid = RandomizedSearchCV(xgb, params, cv=kfold, n_iter=20, n_jobs=-1, random_state=2)\n",
    "        \n",
    "    else:\n",
    "        grid = GridSearchCV(xgb, params, cv=kfold, n_jobs=-1)\n",
    "    \n",
    "    grid.fit(X, y)\n",
    "    best_params = grid.best_params_\n",
    "    print(\"Best params:\", best_params)\n",
    "    best_score = grid.best_score_\n",
    "    print(\"Best score: {:.3f}\".format(best_score))\n",
    "    return best_params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search(params={'n_estimators':[100, 200, 400, 800]})\n",
    "\n",
    "grid_search(params={'learning_rate':[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]})\n",
    "\n",
    "grid_search(params={'max_depth':[2, 3, 5, 6, 8]})\n",
    "\n",
    "grid_search(params={'gamma':[0, 0.01, 0.1, 0.5, 1, 2]})\n",
    "\n",
    "grid_search(params={'min_child_weight':[1, 2, 3, 4, 5]})\n",
    "\n",
    "grid_search(params={'subsample':[0.5, 0.7, 0.8, 0.9, 1]})\n",
    "\n",
    "grid_search(params={'colsample_bytree':[0.5, 0.7, 0.8, 0.9, 1]})\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Early stopping\n",
    "\n",
    "Early stopping is a general method to limit the number of training rounds in iterative machine learning algorithms. In this section, we look at eval_set, eval_metric, and early_stopping_rounds to apply early stopping.\n",
    "\n",
    "Early stopping provides a limit to the number of rounds that iterative machine learning algorithms train on. Instead of predefining the number of training rounds, early stopping allows training to continue until n consecutive rounds fail to produce any gains, where n is a number decided by the user.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### eval_set and eval_metric\n",
    "\n",
    "early_stopping_rounds is not a hyperparameter, but a strategy for optimizing the n_estimators hyperparameter.\n",
    "Normally when choosing hyperparameters, a test score is given after all boosting rounds are complete. To use early stopping, we need a test score after each round."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "\n",
    "model = XGBClassifier(booster='gbtree', objective='binary:logistic',\n",
    "                      radom_state=2, verbosity = 0,  use_label_encoder =False)\n",
    "\n",
    "eval_set = [(X_test, y_test)]\n",
    "\n",
    "eval_metric = 'error'\n",
    "\n",
    "model.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# early stopping rounds\n",
    "model = XGBClassifier(booster='gbtree', objective='binary:logistic',\n",
    "                      random_state=2, use_label_encoder =False)\n",
    "eval_set = [(X_test, y_test)]\n",
    "eval_metric = 'error'\n",
    "\n",
    "model.fit(X_train, y_train, eval_metric='error', eval_set=eval_set,\n",
    "          early_stopping_rounds=10, verbose=True)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = XGBClassifier(random_state=2, n_estimators=5000, use_label_encoder=False)\n",
    "\n",
    "eval_set = [(X_test, y_test)]\n",
    "\n",
    "eval_metric=\"error\"\n",
    "\n",
    "model.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set, early_stopping_rounds=100)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search(params={'n_estimators':[2, 25, 50, 75, 100]});"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = {'max_depth':[1, 2, 3, 4, 5, 6, 7, 8], 'n_estimators': [25]}\n",
    "grid_search(params=params);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = {'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5],'max_depth':[1], 'n_estimators': [25]}\n",
    "grid_search(params=params);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = {'min_child_weight':[1],'learning_rate': [0.5],'max_depth':[1], 'n_estimators': [25]}\n",
    "grid_search(params=params);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = {'subsample':[0.5, 0.6, 0.7, 0.8, 0.9, 1], 'min_child_weight':[1],\n",
    "          'learning_rate': [0.5],'max_depth':[1], 'n_estimators': [25]}\n",
    "grid_search(params=params);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params_ =  {'learning_rate': 0.5, 'max_depth': 1, 'min_child_weight': 1,\n",
    "            'n_estimators': 25, 'subsample': 0.9}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params={'subsample':[0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "                    'min_child_weight':[1, 2, 3, 4, 5],\n",
    "                    'learning_rate':[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                    'max_depth':[1, 2, 3, 4, 5],\n",
    "                    'n_estimators':[2]}\n",
    "grid_search(params);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Hyperparameter adjustments\n",
    "\n",
    "params = {'subsample':[0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "                    'min_child_weight':[1, 2, 3, 4, 5],\n",
    "                    'learning_rate':[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                    'max_depth':[1, 2, 3, 4, 5, None],\n",
    "                    'n_estimators':[2, 25, 50, 75, 100]}\n",
    "grid_search(params=params, random=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Colsample \n",
    "params={'colsample_bytree':[0.5, 0.6, 0.7, 0.8, 0.9, 1], 'max_depth':[1], 'n_estimators':[50]}\n",
    "grid_search(params=params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# colsample_bylevel\n",
    "params={'colsample_bylevel':[0.5, 0.6, 0.7, 0.8, 0.9, 1],'max_depth':[1], 'n_estimators':[50]}\n",
    "grid_search(params=params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# gamma\n",
    "params={'gamma':[0, 0.01, 0.05, 0.1, 0.5, 1, 2, 3], \n",
    "        'colsample_bylevel':[0.9], 'colsample_bytree':[0.8], \n",
    "        'colsample_bynode':[0.5], 'max_depth':[1], 'n_estimators':[25]}\n",
    "grid_search(params=params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discovering Expplanets with XGBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('data/exoplanets.csv', nrows=400)\n",
    "df.head()\n",
    "\n",
    "df['LABEL'].value_counts()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_col = 'LABEL'\n",
    "main_cols_expo = df.columns.difference([target_col])\n",
    "\n",
    "X = df[main_cols_expo].squeeze()\n",
    "y = df[target_col].squeeze()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def light_plot(index):\n",
    "    y_vals = X.iloc[index]\n",
    "    x_vals = np.arange(len(y_vals))\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.xlabel('Number of Observations')\n",
    "    plt.ylabel('Light Flux')\n",
    "    plt.title('Light Plot ' + str(index), size=15)\n",
    "    plt.plot(x_vals, y_vals)\n",
    "    plt.show()\n",
    "\n",
    "light_plot(0)\n",
    "light_plot(1)\n",
    "light_plot(37)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# data prep. \n",
    "df.info()\n",
    "df.isnull().sum().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=2, use_label_encoder=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print('Score: ' + str(score))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))\n",
    "recall_score(y_test, y_pred, pos_label=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resampling imbalanced data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def xgb_clf(model, nrows):\n",
    "    df = pd.read_csv('data/exoplanets.csv', nrows=nrows)\n",
    "    X = df.iloc[:, 1:]\n",
    "    y = df.iloc[:, 0]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = recall_score(y_test, y_pred, pos_label=2)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return score\n",
    "\n",
    "xgb_clf(XGBClassifier(random_state=2), nrows=1000)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xgb_clf(XGBClassifier(random_state=2), nrows=200)\n",
    "\n",
    "xgb_clf(XGBClassifier(random_state=2), nrows=74)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## oversampling\n",
    "df_train = pd.merge(y_train, X_train, left_index=True, right_index=True)\n",
    "df_train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "new_df=pd.DataFrame(np.repeat(df_train[df_train['LABEL'] == 2].values, 9, axis=0))\n",
    "new_df.columns = df_train.columns\n",
    "\n",
    "df_train_resample = pd.concat([df_train, new_df])\n",
    "df_train_resample['LABEL'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train_resample = df_train_resample.iloc[:,1:]\n",
    "y_train_resample = df_train_resample.iloc[:,0]\n",
    "\n",
    "model = XGBClassifier(random_state=2)\n",
    "model.fit(X_train_resample, y_train_resample)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "score = recall_score(y_test, y_pred, pos_label=2)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Tuning and scaling XGBClassifier\n",
    "\n",
    "### Adjusting weights\n",
    "\n",
    "df['LABEL'] = df['LABEL'].replace(1, 0)  # re-assign 1 to 0\n",
    "df['LABEL'] = df['LABEL'].replace(2, 1)  # re-assign 2 to 1\n",
    "\n",
    "df['LABEL'].value_counts()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# scale_pos_weight\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "\n",
    "model = XGBClassifier(scale_pos_weight=10, random_state=2)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "score = recall_score(y_test, y_pred)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The oversampling method that we implemented from scratch gives the same predictions as XGBClassifier with scale_pos_weight.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tuning XGBClassifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=2)\n",
    "model = XGBClassifier(scale_pos_weight=10, random_state=2)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=kfold, scoring='recall')\n",
    "print('Recall: ', scores)\n",
    "print('Recall mean: ', scores.mean())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def grid_search_recall(params: dict, random: bool =False, X=X, y=y, \n",
    "                       model=XGBClassifier(booster='gbtree',\n",
    "                                           objective='binary:logistic', \n",
    "                                           random_state=2, use_label_encoder =False),\n",
    "                       kfold=kfold) -> dict:\n",
    "    xgb = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=2, use_label_encoder =False)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2) \n",
    "    if random:\n",
    "        grid = RandomizedSearchCV(xgb, params, cv=kfold, n_iter=20, n_jobs=-1, random_state=2, scoring='recall')\n",
    "        \n",
    "    else:\n",
    "        grid = GridSearchCV(xgb, params, cv=kfold, n_jobs=-1, scoring='recall')\n",
    "    \n",
    "    grid.fit(X, y)\n",
    "    best_params = grid.best_params_\n",
    "    print(\"Best params:\", best_params)\n",
    "    best_score = grid.best_score_\n",
    "    print(\"Best score: {:.3f}\".format(best_score))\n",
    "    return best_params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search_recall(params={'n_estimators':[50, 200, 400, 800]})\n",
    "grid_search_recall(params={'learning_rate':[0.01, 0.05, 0.2, 0.3]})\n",
    "grid_search_recall(params={'max_depth':[1, 2, 4, 8]})\n",
    "grid_search_recall(params={'subsample':[0.3, 0.5, 0.7, 0.9]})\n",
    "grid_search_recall(params={'gamma':[0.05, 0.1, 0.5, 1]})\n",
    "\n",
    "grid_search_recall(params={'learning_rate':[0.001, 0.01, 0.03], 'max_depth':[1, 2], 'gamma':[0.025, 0.05, 0.5]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search_recall(params={'max_delta_step':[1, 3, 5, 7]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search_recall(params={'subsample':[0.3, 0.5, 0.7, 0.9, 1],\n",
    "'colsample_bylevel':[0.3, 0.5, 0.7, 0.9, 1],\n",
    "'colsample_bynode':[0.3, 0.5, 0.7, 0.9, 1],\n",
    "'colsample_bytree':[0.3, 0.5, 0.7, 0.9, 1]}, random=True);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fine-tuning all data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_all = pd.read_csv('data/exoplanets.csv')\n",
    "df_all.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# replacing 1s with 0s and 2s with 1s\n",
    "df_all['LABEL'] = df_all['LABEL'].replace({1:0, 2:1})\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_all = df_all.iloc[:, 1:]\n",
    "y_all = df_all.iloc[:, 0]\n",
    "\n",
    "df_all['LABEL'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weight = int(df_all['LABEL'].value_counts()[0]/df_all['LABEL'].value_counts()[1])\n",
    "\n",
    "model = XGBClassifier(scale_pos_weight=weight, random_state=2, use_label_encoder=False)\n",
    "scores = cross_val_score(model, X_all, y_all, cv=kfold, scoring='recall')\n",
    "\n",
    "print('Recall:', scores)\n",
    "print('Recall mean:', scores.mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search_recall(params={'learning_rate': [0.001, 0.01]},\n",
    "                   X=X_all, y=y_all,\n",
    "                   model=XGBClassifier(scale_pos_weight=weight, random_state=2))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search_recall(params={'max_depth':[1, 2],'learning_rate': [0.001]},\n",
    "                   X=X_all, y=y_all,\n",
    "                   model=XGBClassifier(scale_pos_weight=weight, random_state=2))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def final_model(X, y, model):\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X_all)\n",
    "    score = recall_score(y_all, y_pred,)\n",
    "    print(score)\n",
    "    print(confusion_matrix(y_all, y_pred,))\n",
    "    print(classification_report(y_all, y_pred))\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# final_model(X_short, y_short, XGBClassifier(max_depth=2, colsample_by_node=0.5, random_state=2))\n",
    "final_model(X, y, XGBClassifier(max_depth=2, colsample_bynode=0.5, scale_pos_weight=10, random_state=2))\n",
    "\n",
    "final_model(X_all, y_all, XGBClassifier(max_depth=2, colsample_bynode=0.5, scale_pos_weight=weight, random_state=2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Advanced XGBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "from sklearn import datasets\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier, XGBRFRegressor, XGBRFClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Applying gblinear via Diabetes datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def regression_model(model, X=X, y=y, kfold=kfold):\n",
    "    scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "    rmse = (-scores)**0.5\n",
    "    print(f\"{model.__class__.__name__} has RMSE: {rmse.mean()} \")\n",
    "    return rmse.mean()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "regression_model(XGBRegressor(booster='gblinear'))\n",
    "regression_model(LinearRegression())\n",
    "regression_model(Lasso())\n",
    "regression_model(Ridge())\n",
    "regression_model(XGBRegressor(booster='gbtree'))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# gblinear hpyerparameters\n",
    "\n",
    "def grid_search_gb(params, X=X, y=y, reg=XGBRegressor(booster='gblinear'),kfold=kfold):\n",
    "    grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=kfold)\n",
    "    grid_reg.fit(X, y)\n",
    "    best_params = grid_reg.best_params_\n",
    "    best_score = np.sqrt(-grid_reg.best_score_)\n",
    "    print(\"Best params:\", best_params)\n",
    "    print(\"Best score:\", best_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search_gb(params={'reg_alpha':[0.001, 0.01, 0.1, 0.5, 1, 5]})\n",
    "grid_search_gb(params={'reg_lambda':[0.001, 0.01, 0.1, 0.5, 1, 5]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search_gb(params={'feature_selector':['shuffle']})\n",
    "grid_search_gb(params={'feature_selector':['random', 'greedy', 'thrifty'], 'updater':['coord_descent'] })\n",
    "grid_search_gb(params={'feature_selector':['greedy', 'thrifty'], 'updater':['coord_descent'], 'top_k':[3, 5, 7, 9]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Linear datasets \n",
    "X = np.arange(1,100)\n",
    "np.random.seed(2) \n",
    "y = []\n",
    "for i in X:\n",
    "    y.append(i*np.random.uniform(-0.2, 0.2))\n",
    "y = np.array(y)\n",
    "X = X.reshape(X.shape[0], 1)\n",
    "y = y.reshape(y.shape[0], 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "regression_model(XGBRegressor(booster='gblinear', objective='reg:squarederror'))\n",
    "\n",
    "regression_model(XGBRegressor(booster='gbtree', objective='reg:squarederror'))\n",
    "\n",
    "regression_model(LinearRegression())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "regression_model(XGBRegressor(booster='dart', objective='reg:squarederror'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# trying dart on bigger dataset \n",
    "df_census = pd.read_csv('data/census_cleaned.csv')\n",
    "\n",
    "X_census = df_census.iloc[:, :-1]\n",
    "y_census = df_census.iloc[:, -1]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def classification_model(model, X=X, y=y,kfold=kfold):\n",
    "    scores = cross_val_score(model, X_census, y_census, scoring='accuracy', cv=kfold)\n",
    "    print(f\"{model.__class__.__name__} has RMSE: {scores.mean()} \")\n",
    "    return scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classification_model(XGBClassifier(booster='gbtree', use_label_encoder=False))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classification_model(XGBClassifier(booster='dart', use_label_encoder=False))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classification_model(XGBClassifier(booster='gblinear', use_label_encoder=False))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DART hyperparameters\n",
    "classification_model(XGBClassifier(booster='dart', one_drop=1, use_label_encoder=False))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classification_model(XGBClassifier(booster='dart', objective='reg:squarederror',\n",
    "                                   sample_type='weighted', use_label_encoder=False))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classification_model(XGBClassifier(booster='dart', objective='reg:squarederror',\n",
    "                                   normalize_type='forest', use_label_encoder=False))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classification_model(XGBClassifier(booster='dart', objective='reg:squarederror',\n",
    "                                   one_drop=1, use_label_encoder=False))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search_gb(params={'rate_drop':[0.01, 0.1, 0.2, 0.4]},\n",
    "               reg=XGBRegressor(booster='dart', objective='reg:squarederror', one_drop=1))\n",
    "\n",
    "grid_search_gb(params={'skip_drop':[0.01, 0.1, 0.2, 0.4]},\n",
    "               reg=XGBRegressor(booster='dart', objective='reg:squarederror'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# XGBoost random forest \n",
    "# There are two strategies to implement random forests within XGBoost. The first is to use random \n",
    "# forests as the base learner, the second is to use XGBoost's original random forests, XGBRFRegressor and XGBRFClassifier.\n",
    "# We start with our original theme, random forests as alternative base learners.\n",
    "\n",
    "# RF as base learners. Option I\n",
    "regression_model(XGBRegressor(booster='gbtree', objective='reg:squarederror', num_parallel_tree=25))\n",
    "regression_model(XGBRegressor(booster='gbtree', objective='reg:squarederror', num_parallel_tree=5))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# RF as xgboost models. Option II\n",
    "regression_model(XGBRFRegressor(objective='reg:squarederror'))\n",
    "regression_model(RandomForestRegressor())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classification_model(XGBRFClassifier(use_label_encoder=False))\n",
    "classification_model(RandomForestClassifier())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Kaggle Masters\n",
    "\n",
    "Here is a general approach for validating and testing machine learning models on your own:\n",
    "\n",
    "* Split data into a training set and a hold-out set: Keep the hold-out set away and resist the temptation to look at it.\n",
    "\n",
    "* Split the training set into a training and test set or use cross-validation: Fit new models on the training set and validate the model, going back and forth to improve scores.\n",
    "\n",
    "* After obtaining a final model, test it on the hold-out set: This is the real test of the model. If the score is below expectations, return to step 2 and repeat. Do not—and this is important—use the hold-out set as the new validation set, going back and forth adjusting hyperparameters. When this happens, the model is adjusting itself to match the hold-out set, which defeats the purpose of a hold-out set in the first place.\n",
    "\n",
    "\n",
    "#### Engineering new columns\n",
    "\n",
    "Feature engineering is the process of developing new columns of data from the original columns. The question is not whether you should implement feature engineering, but how much feature engineering you should implement.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import datetime as dt\n",
    "from category_encoders import target_encoder\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df = pd.read_csv('data/cab_rides.csv', nrows=10000)\n",
    "df.head()\n",
    "df.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   distance          10000 non-null  float64\n",
      " 1   cab_type          10000 non-null  object \n",
      " 2   time_stamp        10000 non-null  int64  \n",
      " 3   destination       10000 non-null  object \n",
      " 4   source            10000 non-null  object \n",
      " 5   price             9227 non-null   float64\n",
      " 6   surge_multiplier  10000 non-null  float64\n",
      " 7   id                10000 non-null  object \n",
      " 8   product_id        10000 non-null  object \n",
      " 9   name              10000 non-null  object \n",
      "dtypes: float64(3), int64(1), object(6)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df[df.isna().any(axis=1)]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      distance cab_type     time_stamp    destination         source  price  \\\n",
       "18        1.11     Uber  1543673584211       West End      North End    NaN   \n",
       "31        2.48     Uber  1543794776318  South Station    Beacon Hill    NaN   \n",
       "40        2.94     Uber  1543523885298         Fenway  North Station    NaN   \n",
       "60        1.16     Uber  1544731816318       West End      North End    NaN   \n",
       "69        2.67     Uber  1543583283653    Beacon Hill      North End    NaN   \n",
       "...        ...      ...            ...            ...            ...    ...   \n",
       "9949      1.08     Uber  1543272429665      North End  North Station    NaN   \n",
       "9953      2.46     Uber  1545045010035    Beacon Hill         Fenway    NaN   \n",
       "9965      2.58     Uber  1544815809335    Beacon Hill  South Station    NaN   \n",
       "9985      1.89     Uber  1544695512211    Beacon Hill      North End    NaN   \n",
       "9994      3.05     Uber  1543812777435         Fenway  North Station    NaN   \n",
       "\n",
       "      surge_multiplier                                    id  \\\n",
       "18                 1.0  fa5fb705-03a0-4eb9-82d9-7fe80872f754   \n",
       "31                 1.0  eee70d94-6706-4b95-a8ce-0e34f0fa8f37   \n",
       "40                 1.0  7f47ff53-7cf2-4a6a-8049-83c90e042593   \n",
       "60                 1.0  43abdbe4-ab9e-4f39-afdc-31cfa375dc25   \n",
       "69                 1.0  80db1c49-9d51-4575-a4f4-1ec23b4d3e31   \n",
       "...                ...                                   ...   \n",
       "9949               1.0  74fffcba-da67-42d1-b585-13d546a125be   \n",
       "9953               1.0  18c2e91d-d594-4a22-9be7-0a5829efa4bf   \n",
       "9965               1.0  77adadfb-4ac7-4cdf-aeab-6c4cfe8f7b26   \n",
       "9985               1.0  f2dfa974-f9d1-4e90-a0e6-77f7eea16956   \n",
       "9994               1.0  66e1e3af-6e64-4045-8739-d5f114b21f47   \n",
       "\n",
       "                                product_id  name  \n",
       "18    8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a  Taxi  \n",
       "31    8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a  Taxi  \n",
       "40    8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a  Taxi  \n",
       "60    8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a  Taxi  \n",
       "69    8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a  Taxi  \n",
       "...                                    ...   ...  \n",
       "9949  8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a  Taxi  \n",
       "9953  8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a  Taxi  \n",
       "9965  8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a  Taxi  \n",
       "9985  8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a  Taxi  \n",
       "9994  8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a  Taxi  \n",
       "\n",
       "[773 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>cab_type</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>destination</th>\n",
       "      <th>source</th>\n",
       "      <th>price</th>\n",
       "      <th>surge_multiplier</th>\n",
       "      <th>id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.11</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1543673584211</td>\n",
       "      <td>West End</td>\n",
       "      <td>North End</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fa5fb705-03a0-4eb9-82d9-7fe80872f754</td>\n",
       "      <td>8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a</td>\n",
       "      <td>Taxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.48</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1543794776318</td>\n",
       "      <td>South Station</td>\n",
       "      <td>Beacon Hill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>eee70d94-6706-4b95-a8ce-0e34f0fa8f37</td>\n",
       "      <td>8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a</td>\n",
       "      <td>Taxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2.94</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1543523885298</td>\n",
       "      <td>Fenway</td>\n",
       "      <td>North Station</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7f47ff53-7cf2-4a6a-8049-83c90e042593</td>\n",
       "      <td>8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a</td>\n",
       "      <td>Taxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.16</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1544731816318</td>\n",
       "      <td>West End</td>\n",
       "      <td>North End</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43abdbe4-ab9e-4f39-afdc-31cfa375dc25</td>\n",
       "      <td>8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a</td>\n",
       "      <td>Taxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2.67</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1543583283653</td>\n",
       "      <td>Beacon Hill</td>\n",
       "      <td>North End</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80db1c49-9d51-4575-a4f4-1ec23b4d3e31</td>\n",
       "      <td>8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a</td>\n",
       "      <td>Taxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9949</th>\n",
       "      <td>1.08</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1543272429665</td>\n",
       "      <td>North End</td>\n",
       "      <td>North Station</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74fffcba-da67-42d1-b585-13d546a125be</td>\n",
       "      <td>8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a</td>\n",
       "      <td>Taxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9953</th>\n",
       "      <td>2.46</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1545045010035</td>\n",
       "      <td>Beacon Hill</td>\n",
       "      <td>Fenway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18c2e91d-d594-4a22-9be7-0a5829efa4bf</td>\n",
       "      <td>8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a</td>\n",
       "      <td>Taxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9965</th>\n",
       "      <td>2.58</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1544815809335</td>\n",
       "      <td>Beacon Hill</td>\n",
       "      <td>South Station</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77adadfb-4ac7-4cdf-aeab-6c4cfe8f7b26</td>\n",
       "      <td>8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a</td>\n",
       "      <td>Taxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>1.89</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1544695512211</td>\n",
       "      <td>Beacon Hill</td>\n",
       "      <td>North End</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>f2dfa974-f9d1-4e90-a0e6-77f7eea16956</td>\n",
       "      <td>8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a</td>\n",
       "      <td>Taxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>3.05</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1543812777435</td>\n",
       "      <td>Fenway</td>\n",
       "      <td>North Station</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66e1e3af-6e64-4045-8739-d5f114b21f47</td>\n",
       "      <td>8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a</td>\n",
       "      <td>Taxi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>773 rows × 10 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df.dropna(inplace=True)\n",
    "\n",
    "df.isna().sum().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df['date'] = pd.to_datetime(df['time_stamp']*(10**6))\n",
    "\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   distance cab_type     time_stamp    destination            source  price  \\\n",
       "0      0.44     Lyft  1544952607890  North Station  Haymarket Square    5.0   \n",
       "1      0.44     Lyft  1543284023677  North Station  Haymarket Square   11.0   \n",
       "2      0.44     Lyft  1543366822198  North Station  Haymarket Square    7.0   \n",
       "3      0.44     Lyft  1543553582749  North Station  Haymarket Square   26.0   \n",
       "4      0.44     Lyft  1543463360223  North Station  Haymarket Square    9.0   \n",
       "\n",
       "   surge_multiplier                                    id    product_id  \\\n",
       "0               1.0  424553bb-7174-41ea-aeb4-fe06d4f4b9d7     lyft_line   \n",
       "1               1.0  4bd23055-6827-41c6-b23b-3c491f24e74d  lyft_premier   \n",
       "2               1.0  981a3613-77af-4620-a42a-0c0866077d1e          lyft   \n",
       "3               1.0  c2d88af2-d278-4bfd-a8d0-29ca77cc5512   lyft_luxsuv   \n",
       "4               1.0  e0126e1f-8ca9-4f2e-82b3-50505a09db9a     lyft_plus   \n",
       "\n",
       "           name                    date  \n",
       "0        Shared 2018-12-16 09:30:07.890  \n",
       "1           Lux 2018-11-27 02:00:23.677  \n",
       "2          Lyft 2018-11-28 01:00:22.198  \n",
       "3  Lux Black XL 2018-11-30 04:53:02.749  \n",
       "4       Lyft XL 2018-11-29 03:49:20.223  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>cab_type</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>destination</th>\n",
       "      <th>source</th>\n",
       "      <th>price</th>\n",
       "      <th>surge_multiplier</th>\n",
       "      <th>id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1544952607890</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>424553bb-7174-41ea-aeb4-fe06d4f4b9d7</td>\n",
       "      <td>lyft_line</td>\n",
       "      <td>Shared</td>\n",
       "      <td>2018-12-16 09:30:07.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1543284023677</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4bd23055-6827-41c6-b23b-3c491f24e74d</td>\n",
       "      <td>lyft_premier</td>\n",
       "      <td>Lux</td>\n",
       "      <td>2018-11-27 02:00:23.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1543366822198</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>981a3613-77af-4620-a42a-0c0866077d1e</td>\n",
       "      <td>lyft</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>2018-11-28 01:00:22.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1543553582749</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>c2d88af2-d278-4bfd-a8d0-29ca77cc5512</td>\n",
       "      <td>lyft_luxsuv</td>\n",
       "      <td>Lux Black XL</td>\n",
       "      <td>2018-11-30 04:53:02.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1543463360223</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>e0126e1f-8ca9-4f2e-82b3-50505a09db9a</td>\n",
       "      <td>lyft_plus</td>\n",
       "      <td>Lyft XL</td>\n",
       "      <td>2018-11-29 03:49:20.223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df['month'] = df['date'].dt.month\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['dayofweek'] = df['date'].dt.dayofweek\n",
    "\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   distance cab_type     time_stamp    destination            source  price  \\\n",
       "0      0.44     Lyft  1544952607890  North Station  Haymarket Square    5.0   \n",
       "1      0.44     Lyft  1543284023677  North Station  Haymarket Square   11.0   \n",
       "2      0.44     Lyft  1543366822198  North Station  Haymarket Square    7.0   \n",
       "3      0.44     Lyft  1543553582749  North Station  Haymarket Square   26.0   \n",
       "4      0.44     Lyft  1543463360223  North Station  Haymarket Square    9.0   \n",
       "\n",
       "   surge_multiplier                                    id    product_id  \\\n",
       "0               1.0  424553bb-7174-41ea-aeb4-fe06d4f4b9d7     lyft_line   \n",
       "1               1.0  4bd23055-6827-41c6-b23b-3c491f24e74d  lyft_premier   \n",
       "2               1.0  981a3613-77af-4620-a42a-0c0866077d1e          lyft   \n",
       "3               1.0  c2d88af2-d278-4bfd-a8d0-29ca77cc5512   lyft_luxsuv   \n",
       "4               1.0  e0126e1f-8ca9-4f2e-82b3-50505a09db9a     lyft_plus   \n",
       "\n",
       "           name                    date  month  hour  dayofweek  \n",
       "0        Shared 2018-12-16 09:30:07.890     12     9          6  \n",
       "1           Lux 2018-11-27 02:00:23.677     11     2          1  \n",
       "2          Lyft 2018-11-28 01:00:22.198     11     1          2  \n",
       "3  Lux Black XL 2018-11-30 04:53:02.749     11     4          4  \n",
       "4       Lyft XL 2018-11-29 03:49:20.223     11     3          3  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>cab_type</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>destination</th>\n",
       "      <th>source</th>\n",
       "      <th>price</th>\n",
       "      <th>surge_multiplier</th>\n",
       "      <th>id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1544952607890</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>424553bb-7174-41ea-aeb4-fe06d4f4b9d7</td>\n",
       "      <td>lyft_line</td>\n",
       "      <td>Shared</td>\n",
       "      <td>2018-12-16 09:30:07.890</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1543284023677</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4bd23055-6827-41c6-b23b-3c491f24e74d</td>\n",
       "      <td>lyft_premier</td>\n",
       "      <td>Lux</td>\n",
       "      <td>2018-11-27 02:00:23.677</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1543366822198</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>981a3613-77af-4620-a42a-0c0866077d1e</td>\n",
       "      <td>lyft</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>2018-11-28 01:00:22.198</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1543553582749</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>c2d88af2-d278-4bfd-a8d0-29ca77cc5512</td>\n",
       "      <td>lyft_luxsuv</td>\n",
       "      <td>Lux Black XL</td>\n",
       "      <td>2018-11-30 04:53:02.749</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1543463360223</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>e0126e1f-8ca9-4f2e-82b3-50505a09db9a</td>\n",
       "      <td>lyft_plus</td>\n",
       "      <td>Lyft XL</td>\n",
       "      <td>2018-11-29 03:49:20.223</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def weekend(row):\n",
    "    if row['dayofweek'] in [5, 6]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def rush_hour(row):\n",
    "    if (row['hour'] in [6,7,8,9,15,16,17,18]) & (row['weekend'] == 0):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "df['weekend'] = df.apply(weekend, axis=1)\n",
    "df['rush_hour'] = df.apply(rush_hour, axis=1)\n",
    "df.tail()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   distance cab_type     time_stamp    destination            source  price  \\\n",
       "0      0.44     Lyft  1544952607890  North Station  Haymarket Square    5.0   \n",
       "1      0.44     Lyft  1543284023677  North Station  Haymarket Square   11.0   \n",
       "2      0.44     Lyft  1543366822198  North Station  Haymarket Square    7.0   \n",
       "3      0.44     Lyft  1543553582749  North Station  Haymarket Square   26.0   \n",
       "4      0.44     Lyft  1543463360223  North Station  Haymarket Square    9.0   \n",
       "\n",
       "   surge_multiplier                                    id    product_id  \\\n",
       "0               1.0  424553bb-7174-41ea-aeb4-fe06d4f4b9d7     lyft_line   \n",
       "1               1.0  4bd23055-6827-41c6-b23b-3c491f24e74d  lyft_premier   \n",
       "2               1.0  981a3613-77af-4620-a42a-0c0866077d1e          lyft   \n",
       "3               1.0  c2d88af2-d278-4bfd-a8d0-29ca77cc5512   lyft_luxsuv   \n",
       "4               1.0  e0126e1f-8ca9-4f2e-82b3-50505a09db9a     lyft_plus   \n",
       "\n",
       "           name                    date  month  hour  dayofweek  weekend  \n",
       "0        Shared 2018-12-16 09:30:07.890     12     9          6        1  \n",
       "1           Lux 2018-11-27 02:00:23.677     11     2          1        0  \n",
       "2          Lyft 2018-11-28 01:00:22.198     11     1          2        0  \n",
       "3  Lux Black XL 2018-11-30 04:53:02.749     11     4          4        0  \n",
       "4       Lyft XL 2018-11-29 03:49:20.223     11     3          3        0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>cab_type</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>destination</th>\n",
       "      <th>source</th>\n",
       "      <th>price</th>\n",
       "      <th>surge_multiplier</th>\n",
       "      <th>id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1544952607890</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>424553bb-7174-41ea-aeb4-fe06d4f4b9d7</td>\n",
       "      <td>lyft_line</td>\n",
       "      <td>Shared</td>\n",
       "      <td>2018-12-16 09:30:07.890</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1543284023677</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4bd23055-6827-41c6-b23b-3c491f24e74d</td>\n",
       "      <td>lyft_premier</td>\n",
       "      <td>Lux</td>\n",
       "      <td>2018-11-27 02:00:23.677</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1543366822198</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>981a3613-77af-4620-a42a-0c0866077d1e</td>\n",
       "      <td>lyft</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>2018-11-28 01:00:22.198</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1543553582749</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>c2d88af2-d278-4bfd-a8d0-29ca77cc5512</td>\n",
       "      <td>lyft_luxsuv</td>\n",
       "      <td>Lux Black XL</td>\n",
       "      <td>2018-11-30 04:53:02.749</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.44</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1543463360223</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Haymarket Square</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>e0126e1f-8ca9-4f2e-82b3-50505a09db9a</td>\n",
       "      <td>lyft_plus</td>\n",
       "      <td>Lyft XL</td>\n",
       "      <td>2018-11-29 03:49:20.223</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df.cab_type.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Uber    4654\n",
       "Lyft    4573\n",
       "Name: cab_type, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# grouping by cab_type \n",
    "df['cab_freq'] = df.groupby('cab_type')['cab_type'].transform('count')\n",
    "df['cab_freq'] = df['cab_freq']/len(df)\n",
    "\n",
    "encoder = target_encoder.TargetEncoder()\n",
    "df['cab_type_mean'] = encoder.fit_transform(df['cab_type'], df['price'])\n",
    "df.tail()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      distance cab_type     time_stamp       destination  \\\n",
       "9995      3.05     Uber  1543504379037            Fenway   \n",
       "9996      3.05     Uber  1543800477997            Fenway   \n",
       "9997      3.05     Uber  1543407083241            Fenway   \n",
       "9998      3.05     Uber  1544896813623            Fenway   \n",
       "9999      2.03     Lyft  1543812781166  Theatre District   \n",
       "\n",
       "                       source  price  surge_multiplier  \\\n",
       "9995            North Station   11.5               1.0   \n",
       "9996            North Station   26.0               1.0   \n",
       "9997            North Station   19.5               1.0   \n",
       "9998            North Station   36.5               1.0   \n",
       "9999  Northeastern University    7.0               1.0   \n",
       "\n",
       "                                        id  \\\n",
       "9995  934d2fbe-f978-4495-9786-da7b4dd21107   \n",
       "9996  af8fd57c-fe7c-4584-bd1f-beef1a53ad42   \n",
       "9997  b3c5db97-554b-47bf-908b-3ac880e86103   \n",
       "9998  fcb35184-9047-43f7-8909-f62a7b17b6cf   \n",
       "9999  7f0e8caf-e057-41eb-bdef-27eb14c88122   \n",
       "\n",
       "                                product_id       name                    date  \\\n",
       "9995  997acbb5-e102-41e1-b155-9df7de0a73f2   UberPool 2018-11-29 15:12:59.037   \n",
       "9996  6c84fd89-3f11-4782-9b50-97c468b19529      Black 2018-12-03 01:27:57.997   \n",
       "9997  6f72dfc5-27f1-42e8-84db-ccc7a75f6969     UberXL 2018-11-28 12:11:23.241   \n",
       "9998  6d318bcc-22a3-4af6-bddd-b409bfce1546  Black SUV 2018-12-15 18:00:13.623   \n",
       "9999                             lyft_line     Shared 2018-12-03 04:53:01.166   \n",
       "\n",
       "      month  hour  dayofweek  weekend  rush_hour  cab_freq  \n",
       "9995     11    15          3        0          1  0.504389  \n",
       "9996     12     1          0        0          0  0.504389  \n",
       "9997     11    12          2        0          0  0.504389  \n",
       "9998     12    18          5        1          0  0.504389  \n",
       "9999     12     4          0        0          0  0.495611  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>cab_type</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>destination</th>\n",
       "      <th>source</th>\n",
       "      <th>price</th>\n",
       "      <th>surge_multiplier</th>\n",
       "      <th>id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>weekend</th>\n",
       "      <th>rush_hour</th>\n",
       "      <th>cab_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>3.05</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1543504379037</td>\n",
       "      <td>Fenway</td>\n",
       "      <td>North Station</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>934d2fbe-f978-4495-9786-da7b4dd21107</td>\n",
       "      <td>997acbb5-e102-41e1-b155-9df7de0a73f2</td>\n",
       "      <td>UberPool</td>\n",
       "      <td>2018-11-29 15:12:59.037</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.504389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>3.05</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1543800477997</td>\n",
       "      <td>Fenway</td>\n",
       "      <td>North Station</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>af8fd57c-fe7c-4584-bd1f-beef1a53ad42</td>\n",
       "      <td>6c84fd89-3f11-4782-9b50-97c468b19529</td>\n",
       "      <td>Black</td>\n",
       "      <td>2018-12-03 01:27:57.997</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.504389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>3.05</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1543407083241</td>\n",
       "      <td>Fenway</td>\n",
       "      <td>North Station</td>\n",
       "      <td>19.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b3c5db97-554b-47bf-908b-3ac880e86103</td>\n",
       "      <td>6f72dfc5-27f1-42e8-84db-ccc7a75f6969</td>\n",
       "      <td>UberXL</td>\n",
       "      <td>2018-11-28 12:11:23.241</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.504389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>3.05</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1544896813623</td>\n",
       "      <td>Fenway</td>\n",
       "      <td>North Station</td>\n",
       "      <td>36.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fcb35184-9047-43f7-8909-f62a7b17b6cf</td>\n",
       "      <td>6d318bcc-22a3-4af6-bddd-b409bfce1546</td>\n",
       "      <td>Black SUV</td>\n",
       "      <td>2018-12-15 18:00:13.623</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.504389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>2.03</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>1543812781166</td>\n",
       "      <td>Theatre District</td>\n",
       "      <td>Northeastern University</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7f0e8caf-e057-41eb-bdef-27eb14c88122</td>\n",
       "      <td>lyft_line</td>\n",
       "      <td>Shared</td>\n",
       "      <td>2018-12-03 04:53:01.166</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.495611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Range of models\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "def classification_model(model):\n",
    "    scores = cross_val_score(model, X, y, cv=kfold)\n",
    "    return scores.mean()\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "classification_model(XGBClassifier(verbose=0, silent=True))\n",
    "classification_model(XGBClassifier(booster='gblinear', silent=True))\n",
    "classification_model(XGBClassifier(booster='dart', one_drop=True, silent=True))\n",
    "classification_model(RandomForestClassifier(random_state=2))\n",
    "classification_model(LogisticRegression(max_iter=10000))\n",
    "classification_model(XGBClassifier(max_depth=2, n_estimators=500, learning_rate=0.1, silent=True))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[08:30:31] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\", \"verbose\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:32] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\", \"verbose\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:32] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\", \"verbose\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:33] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\", \"verbose\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:33] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\", \"verbose\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9771619313771154"
      ]
     },
     "metadata": {},
     "execution_count": 23
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[08:30:33] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:33] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:33] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:33] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:33] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9631268436578171"
      ]
     },
     "metadata": {},
     "execution_count": 23
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[08:30:33] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:33] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:34] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:34] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:34] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:34] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:34] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:34] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9683744760130415"
      ]
     },
     "metadata": {},
     "execution_count": 23
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9666356155876418"
      ]
     },
     "metadata": {},
     "execution_count": 23
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9507995652848935"
      ]
     },
     "metadata": {},
     "execution_count": 23
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[08:30:41] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:41] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:42] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:42] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:30:42] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9701133364384411"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Correlation in ML enembles\n",
    "\n",
    "def y_pred(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = accuracy_score(y_pred, y_test)\n",
    "    print(score)\n",
    "    return y_pred\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "y_pred_gbtree = y_pred(XGBClassifier())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[09:32:13] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.951048951048951\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "y_pred_dart = y_pred(XGBClassifier(booster='dart', one_drop=True))\n",
    "y_pred_forest = y_pred(RandomForestClassifier())\n",
    "y_pred_logistic = y_pred(LogisticRegression(max_iter=10000))\n",
    "y_pred_xgb = y_pred(XGBClassifier(max_depth=2, n_estimators=500, learning_rate=0.1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[09:35:43] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.951048951048951\n",
      "0.9370629370629371\n",
      "0.9370629370629371\n",
      "[09:35:47] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.965034965034965\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "df_pred = pd.DataFrame(data= np.c_[y_pred_gbtree, y_pred_dart, y_pred_forest, y_pred_logistic, y_pred_xgb],\n",
    "                       columns=['gbtree', 'dart','forest', 'logistic', 'xgb'])\n",
    "df_pred.corr()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            gbtree      dart    forest  logistic       xgb\n",
       "gbtree    1.000000  0.971146  0.913384  0.914111  0.971146\n",
       "dart      0.971146  1.000000  0.942396  0.914111  0.971146\n",
       "forest    0.913384  0.942396  1.000000  0.941715  0.913384\n",
       "logistic  0.914111  0.914111  0.941715  1.000000  0.914111\n",
       "xgb       0.971146  0.971146  0.913384  0.914111  1.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gbtree</th>\n",
       "      <th>dart</th>\n",
       "      <th>forest</th>\n",
       "      <th>logistic</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gbtree</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971146</td>\n",
       "      <td>0.913384</td>\n",
       "      <td>0.914111</td>\n",
       "      <td>0.971146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dart</th>\n",
       "      <td>0.971146</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942396</td>\n",
       "      <td>0.914111</td>\n",
       "      <td>0.971146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest</th>\n",
       "      <td>0.913384</td>\n",
       "      <td>0.942396</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941715</td>\n",
       "      <td>0.913384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic</th>\n",
       "      <td>0.914111</td>\n",
       "      <td>0.914111</td>\n",
       "      <td>0.941715</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>0.971146</td>\n",
       "      <td>0.971146</td>\n",
       "      <td>0.913384</td>\n",
       "      <td>0.914111</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# The VotingClassifier ensemble\n",
    "\n",
    "estimators = []\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=10000)\n",
    "estimators.append(('logistic', logistic_model))\n",
    "\n",
    "xgb_model = XGBClassifier(max_depth=2, n_estimators=500, learning_rate=0.1)\n",
    "estimators.append(('xgb', xgb_model))\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=2)\n",
    "estimators.append(('rf', rf_model))\n",
    "\n",
    "ensemble = VotingClassifier(estimators)\n",
    "\n",
    "scores = cross_val_score(ensemble, X, y, cv=kfold)\n",
    "print(scores.mean())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[09:39:48] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:39:52] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:39:54] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:39:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:39:59] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9771619313771154\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# stacking using sklearn\n",
    "base_models = []\n",
    "\n",
    "base_models.append(('lr', LogisticRegression()))\n",
    "base_models.append(('xgb', XGBClassifier()))\n",
    "base_models.append(('rf', RandomForestClassifier(random_state=2)))\n",
    "\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "clf = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "scores = cross_val_score(clf, X, y, cv=kfold)\n",
    "print(scores.mean())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[09:44:17] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:22] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:24] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:24] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:24] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:24] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:24] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:25] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:25] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:26] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:26] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:26] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:26] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:44:26] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9789318428815401\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost Model Deployment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "import pandas as pd \n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df = pd.read_csv('data/student-por.csv', sep=';')\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  school  sex   age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
       "0     GP  NaN  18.0       U     GT3       A     4     4  at_home   teacher   \n",
       "1     GP    F   NaN       U     GT3       T     1     1  at_home     other   \n",
       "2     GP    F  15.0       U     LE3       T     1     1  at_home     other   \n",
       "3     GP    F  15.0       U     GT3       T     4     2   health  services   \n",
       "4     GP    F  16.0       U     GT3       T     3     3    other     other   \n",
       "\n",
       "   ... famrel freetime  goout  Dalc  Walc health absences  G1  G2  G3  \n",
       "0  ...      4        3      4     1     1      3        4   0  11  11  \n",
       "1  ...      5        3      3     1     1      3        2   9  11  11  \n",
       "2  ...      4        3      2     2     3      3        6  12  13  12  \n",
       "3  ...      3        2      2     1     1      5        0  14  14  14  \n",
       "4  ...      4        3      2     1     2      5        0  11  13  13  \n",
       "\n",
       "[5 rows x 33 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15.0</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15.0</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16.0</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df.info()\n",
    "df.isnull().sum().sum()\n",
    "\n",
    "df[df.isna().any(axis=1)]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 649 entries, 0 to 648\n",
      "Data columns (total 33 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   school      649 non-null    object \n",
      " 1   sex         648 non-null    object \n",
      " 2   age         648 non-null    float64\n",
      " 3   address     649 non-null    object \n",
      " 4   famsize     649 non-null    object \n",
      " 5   Pstatus     649 non-null    object \n",
      " 6   Medu        649 non-null    int64  \n",
      " 7   Fedu        649 non-null    int64  \n",
      " 8   Mjob        649 non-null    object \n",
      " 9   Fjob        649 non-null    object \n",
      " 10  reason      649 non-null    object \n",
      " 11  guardian    648 non-null    object \n",
      " 12  traveltime  649 non-null    int64  \n",
      " 13  studytime   649 non-null    int64  \n",
      " 14  failures    649 non-null    int64  \n",
      " 15  schoolsup   649 non-null    object \n",
      " 16  famsup      649 non-null    object \n",
      " 17  paid        649 non-null    object \n",
      " 18  activities  649 non-null    object \n",
      " 19  nursery     649 non-null    object \n",
      " 20  higher      649 non-null    object \n",
      " 21  internet    649 non-null    object \n",
      " 22  romantic    649 non-null    object \n",
      " 23  famrel      649 non-null    int64  \n",
      " 24  freetime    649 non-null    int64  \n",
      " 25  goout       649 non-null    int64  \n",
      " 26  Dalc        649 non-null    int64  \n",
      " 27  Walc        649 non-null    int64  \n",
      " 28  health      649 non-null    int64  \n",
      " 29  absences    649 non-null    int64  \n",
      " 30  G1          649 non-null    int64  \n",
      " 31  G2          649 non-null    int64  \n",
      " 32  G3          649 non-null    int64  \n",
      "dtypes: float64(1), int64(15), object(17)\n",
      "memory usage: 167.4+ KB\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  school  sex   age address famsize Pstatus  Medu  Fedu     Mjob     Fjob  \\\n",
       "0     GP  NaN  18.0       U     GT3       A     4     4  at_home  teacher   \n",
       "1     GP    F   NaN       U     GT3       T     1     1  at_home    other   \n",
       "\n",
       "   ... famrel freetime  goout  Dalc  Walc health absences G1  G2  G3  \n",
       "0  ...      4        3      4     1     1      3        4  0  11  11  \n",
       "1  ...      5        3      3     1     1      3        2  9  11  11  \n",
       "\n",
       "[2 rows x 33 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 33 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# filling na's\n",
    "df['age'].fillna(-999.0)\n",
    "df['sex'] = df['sex'].fillna(df['sex'].mode())\n",
    "df['guardian'] = df['guardian'].fillna(df['guardian'].mode())\n",
    "\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  school sex   age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
       "0     GP   F  18.0       U     GT3       A     4     4  at_home   teacher   \n",
       "1     GP   F   NaN       U     GT3       T     1     1  at_home     other   \n",
       "2     GP   F  15.0       U     LE3       T     1     1  at_home     other   \n",
       "3     GP   F  15.0       U     GT3       T     4     2   health  services   \n",
       "4     GP   F  16.0       U     GT3       T     3     3    other     other   \n",
       "\n",
       "   ... famrel freetime  goout  Dalc  Walc health absences  G1  G2  G3  \n",
       "0  ...      4        3      4     1     1      3        4   0  11  11  \n",
       "1  ...      5        3      3     1     1      3        2   9  11  11  \n",
       "2  ...      4        3      2     2     3      3        6  12  13  12  \n",
       "3  ...      3        2      2     1     1      5        0  14  14  14  \n",
       "4  ...      4        3      2     1     2      5        0  11  13  13  \n",
       "\n",
       "[5 rows x 33 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18.0</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15.0</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15.0</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16.0</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# one hot encoding \n",
    "cat_cols = df.columns[df.dtypes == object].tolist()\n",
    "num_cols = df.columns.difference(cat_cols).tolist()\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "hot = ohe.fit_transform(df[cat_cols])\n",
    "hot_df = pd.DataFrame(hot.toarray())\n",
    "\n",
    "hot_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9   ...   33   34   35   36  \\\n",
       "0  1.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  1.0  0.0  ...  1.0  0.0  0.0  1.0   \n",
       "1  1.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  ...  1.0  0.0  1.0  0.0   \n",
       "2  1.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0  1.0  ...  1.0  0.0  0.0  1.0   \n",
       "3  1.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  ...  0.0  1.0  0.0  1.0   \n",
       "4  1.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  ...  1.0  0.0  0.0  1.0   \n",
       "\n",
       "    37   38   39   40   41   42  \n",
       "0  0.0  1.0  1.0  0.0  1.0  0.0  \n",
       "1  0.0  1.0  0.0  1.0  1.0  0.0  \n",
       "2  0.0  1.0  0.0  1.0  1.0  0.0  \n",
       "3  0.0  1.0  0.0  1.0  0.0  1.0  \n",
       "4  0.0  1.0  1.0  0.0  1.0  0.0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "cold_df = df.select_dtypes(exclude=['object'])\n",
    "cold_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    age  Medu  Fedu  traveltime  studytime  failures  famrel  freetime  goout  \\\n",
       "0  18.0     4     4           2          2         0       4         3      4   \n",
       "1   NaN     1     1           1          2         0       5         3      3   \n",
       "2  15.0     1     1           1          2         0       4         3      2   \n",
       "3  15.0     4     2           1          3         0       3         2      2   \n",
       "4  16.0     3     3           1          2         0       4         3      2   \n",
       "\n",
       "   Dalc  Walc  health  absences  G1  G2  G3  \n",
       "0     1     1       3         4   0  11  11  \n",
       "1     1     1       3         2   9  11  11  \n",
       "2     2     3       3         6  12  13  12  \n",
       "3     1     1       5         0  14  14  14  \n",
       "4     1     2       5         0  11  13  13  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "cold_df[num_cols].head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Dalc  Fedu  G1  G2  G3  Medu  Walc  absences   age  failures  famrel  \\\n",
       "0     1     4   0  11  11     4     1         4  18.0         0       4   \n",
       "1     1     1   9  11  11     1     1         2   NaN         0       5   \n",
       "2     2     1  12  13  12     1     3         6  15.0         0       4   \n",
       "3     1     2  14  14  14     4     1         0  15.0         0       3   \n",
       "4     1     3  11  13  13     3     2         0  16.0         0       4   \n",
       "\n",
       "   freetime  goout  health  studytime  traveltime  \n",
       "0         3      4       3          2           2  \n",
       "1         3      3       3          2           1  \n",
       "2         3      2       3          2           1  \n",
       "3         2      2       5          3           1  \n",
       "4         3      2       5          2           1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Walc</th>\n",
       "      <th>absences</th>\n",
       "      <th>age</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>health</th>\n",
       "      <th>studytime</th>\n",
       "      <th>traveltime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "cold = csr_matrix(cold_df)\n",
    "final_sparse_matrix = hstack((hot, cold))\n",
    "\n",
    "final_df = pd.DataFrame(final_sparse_matrix.toarray())\n",
    "final_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9   ...   49   50   51   52  \\\n",
       "0  1.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  1.0  0.0  ...  4.0  3.0  4.0  1.0   \n",
       "1  1.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  ...  5.0  3.0  3.0  1.0   \n",
       "2  1.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0  1.0  ...  4.0  3.0  2.0  2.0   \n",
       "3  1.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  ...  3.0  2.0  2.0  1.0   \n",
       "4  1.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  ...  4.0  3.0  2.0  1.0   \n",
       "\n",
       "    53   54   55    56    57    58  \n",
       "0  1.0  3.0  4.0   0.0  11.0  11.0  \n",
       "1  1.0  3.0  2.0   9.0  11.0  11.0  \n",
       "2  3.0  3.0  6.0  12.0  13.0  12.0  \n",
       "3  1.0  5.0  0.0  14.0  14.0  14.0  \n",
       "4  2.0  5.0  0.0  11.0  13.0  13.0  \n",
       "\n",
       "[5 rows x 59 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# custom transformers \n",
    "class NullValueImputer(TransformerMixin):\n",
    "    def __init__(self) -> None:\n",
    "        None\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        for column in X.columns.tolist():\n",
    "            if column in X.columns[X.dtypes==object].tolist():\n",
    "                X[column] = X[column].fillna(X[column].mode())\n",
    "            else:\n",
    "                X[column]=X[column].fillna(-999.0)\n",
    "        return X\n",
    "\n",
    "class SparseMatrix(TransformerMixin):\n",
    "    def __init__(self) -> None:\n",
    "        None\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        cat_cols = X.columns[X.dtypes==object].tolist()\n",
    "        hot = OneHotEncoder().fit_transform(X[cat_cols])\n",
    "        cold = csr_matrix(X.select_dtypes(exclude=[\"object\"]))\n",
    "        final_sparse_matrix = hstack((hot, cold))\n",
    "        final_csr_matrix = final_sparse_matrix.tocsr()\n",
    "        final_df = pd.DataFrame(final_csr_matrix.toarray())\n",
    "        return final_df\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df = pd.read_csv('data/student-por.csv', sep=';')\n",
    "nvi = NullValueImputer().fit_transform(df)\n",
    "nvi.head()\n",
    "\n",
    "sm = SparseMatrix().fit_transform(df)\n",
    "sm.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9   ...   49   50   51   52  \\\n",
       "0  1.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  1.0  0.0  ...  4.0  3.0  4.0  1.0   \n",
       "1  1.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  ...  5.0  3.0  3.0  1.0   \n",
       "2  1.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0  1.0  ...  4.0  3.0  2.0  2.0   \n",
       "3  1.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  ...  3.0  2.0  2.0  1.0   \n",
       "4  1.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  ...  4.0  3.0  2.0  1.0   \n",
       "\n",
       "    53   54   55    56    57    58  \n",
       "0  1.0  3.0  4.0   0.0  11.0  11.0  \n",
       "1  1.0  3.0  2.0   9.0  11.0  11.0  \n",
       "2  3.0  3.0  6.0  12.0  13.0  12.0  \n",
       "3  1.0  5.0  0.0  14.0  14.0  14.0  \n",
       "4  2.0  5.0  0.0  11.0  13.0  13.0  \n",
       "\n",
       "[5 rows x 59 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "df = pd.read_csv('data/student-por.csv', sep=';')\n",
    "\n",
    "y = df.iloc[:, -1]\n",
    "X = df.iloc[:, :-3]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "\n",
    "data_pipeline = Pipeline([('null_imputer', NullValueImputer()), ('sparse', SparseMatrix())])\n",
    "X_train_transformed = data_pipeline.fit_transform(X_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "y_train.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "11    82\n",
       "10    75\n",
       "13    58\n",
       "12    53\n",
       "14    42\n",
       "15    36\n",
       "9     29\n",
       "16    27\n",
       "8     26\n",
       "17    24\n",
       "18    14\n",
       "0     10\n",
       "7      7\n",
       "5      1\n",
       "6      1\n",
       "19     1\n",
       "Name: G3, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=2)\n",
    "\n",
    "def cross_val(model):\n",
    "    scores = cross_val_score(model, X_train_transformed, y_train, scoring='neg_root_mean_squared_error', cv=kfold)\n",
    "    rmse = (-scores.mean())\n",
    "    return rmse\n",
    "\n",
    "cross_val(XGBRegressor(missing=-999.0))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.9004041754792746"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_train_transformed, y_train, random_state=2)\n",
    "def n_estimators(model):\n",
    "    eval_set = [(X_test_2, y_test_2)]\n",
    "    eval_metric=\"rmse\"\n",
    "    model.fit(X_train_2, y_train_2, eval_metric=eval_metric, eval_set=eval_set, early_stopping_rounds=100)\n",
    "    y_pred = model.predict(X_test_2)\n",
    "    rmse = MSE(y_test_2, y_pred)**0.5\n",
    "    return rmse\n",
    "\n",
    "n_estimators(XGBRegressor(n_estimators=5000, missing=-999.0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0]\tvalidation_0-rmse:8.49176\n",
      "[1]\tvalidation_0-rmse:6.31389\n",
      "[2]\tvalidation_0-rmse:4.97965\n",
      "[3]\tvalidation_0-rmse:4.16109\n",
      "[4]\tvalidation_0-rmse:3.67782\n",
      "[5]\tvalidation_0-rmse:3.42779\n",
      "[6]\tvalidation_0-rmse:3.30579\n",
      "[7]\tvalidation_0-rmse:3.25238\n",
      "[8]\tvalidation_0-rmse:3.22878\n",
      "[9]\tvalidation_0-rmse:3.20020\n",
      "[10]\tvalidation_0-rmse:3.17934\n",
      "[11]\tvalidation_0-rmse:3.16766\n",
      "[12]\tvalidation_0-rmse:3.15062\n",
      "[13]\tvalidation_0-rmse:3.13508\n",
      "[14]\tvalidation_0-rmse:3.14204\n",
      "[15]\tvalidation_0-rmse:3.13769\n",
      "[16]\tvalidation_0-rmse:3.15551\n",
      "[17]\tvalidation_0-rmse:3.15064\n",
      "[18]\tvalidation_0-rmse:3.14732\n",
      "[19]\tvalidation_0-rmse:3.14887\n",
      "[20]\tvalidation_0-rmse:3.14607\n",
      "[21]\tvalidation_0-rmse:3.14591\n",
      "[22]\tvalidation_0-rmse:3.14349\n",
      "[23]\tvalidation_0-rmse:3.14303\n",
      "[24]\tvalidation_0-rmse:3.14024\n",
      "[25]\tvalidation_0-rmse:3.14376\n",
      "[26]\tvalidation_0-rmse:3.14765\n",
      "[27]\tvalidation_0-rmse:3.14520\n",
      "[28]\tvalidation_0-rmse:3.13969\n",
      "[29]\tvalidation_0-rmse:3.14365\n",
      "[30]\tvalidation_0-rmse:3.13755\n",
      "[31]\tvalidation_0-rmse:3.14122\n",
      "[32]\tvalidation_0-rmse:3.13680\n",
      "[33]\tvalidation_0-rmse:3.12997\n",
      "[34]\tvalidation_0-rmse:3.12537\n",
      "[35]\tvalidation_0-rmse:3.13007\n",
      "[36]\tvalidation_0-rmse:3.12821\n",
      "[37]\tvalidation_0-rmse:3.13539\n",
      "[38]\tvalidation_0-rmse:3.13887\n",
      "[39]\tvalidation_0-rmse:3.13982\n",
      "[40]\tvalidation_0-rmse:3.14055\n",
      "[41]\tvalidation_0-rmse:3.14010\n",
      "[42]\tvalidation_0-rmse:3.13791\n",
      "[43]\tvalidation_0-rmse:3.13999\n",
      "[44]\tvalidation_0-rmse:3.13948\n",
      "[45]\tvalidation_0-rmse:3.14186\n",
      "[46]\tvalidation_0-rmse:3.14134\n",
      "[47]\tvalidation_0-rmse:3.14076\n",
      "[48]\tvalidation_0-rmse:3.14304\n",
      "[49]\tvalidation_0-rmse:3.14090\n",
      "[50]\tvalidation_0-rmse:3.14188\n",
      "[51]\tvalidation_0-rmse:3.14097\n",
      "[52]\tvalidation_0-rmse:3.14126\n",
      "[53]\tvalidation_0-rmse:3.14017\n",
      "[54]\tvalidation_0-rmse:3.13996\n",
      "[55]\tvalidation_0-rmse:3.14009\n",
      "[56]\tvalidation_0-rmse:3.13925\n",
      "[57]\tvalidation_0-rmse:3.13745\n",
      "[58]\tvalidation_0-rmse:3.13710\n",
      "[59]\tvalidation_0-rmse:3.13733\n",
      "[60]\tvalidation_0-rmse:3.13675\n",
      "[61]\tvalidation_0-rmse:3.13487\n",
      "[62]\tvalidation_0-rmse:3.13382\n",
      "[63]\tvalidation_0-rmse:3.13414\n",
      "[64]\tvalidation_0-rmse:3.13416\n",
      "[65]\tvalidation_0-rmse:3.13468\n",
      "[66]\tvalidation_0-rmse:3.13477\n",
      "[67]\tvalidation_0-rmse:3.13514\n",
      "[68]\tvalidation_0-rmse:3.13466\n",
      "[69]\tvalidation_0-rmse:3.13483\n",
      "[70]\tvalidation_0-rmse:3.13417\n",
      "[71]\tvalidation_0-rmse:3.13390\n",
      "[72]\tvalidation_0-rmse:3.13388\n",
      "[73]\tvalidation_0-rmse:3.13372\n",
      "[74]\tvalidation_0-rmse:3.13348\n",
      "[75]\tvalidation_0-rmse:3.13364\n",
      "[76]\tvalidation_0-rmse:3.13342\n",
      "[77]\tvalidation_0-rmse:3.13386\n",
      "[78]\tvalidation_0-rmse:3.13367\n",
      "[79]\tvalidation_0-rmse:3.13348\n",
      "[80]\tvalidation_0-rmse:3.13347\n",
      "[81]\tvalidation_0-rmse:3.13351\n",
      "[82]\tvalidation_0-rmse:3.13334\n",
      "[83]\tvalidation_0-rmse:3.13399\n",
      "[84]\tvalidation_0-rmse:3.13412\n",
      "[85]\tvalidation_0-rmse:3.13418\n",
      "[86]\tvalidation_0-rmse:3.13451\n",
      "[87]\tvalidation_0-rmse:3.13453\n",
      "[88]\tvalidation_0-rmse:3.13468\n",
      "[89]\tvalidation_0-rmse:3.13468\n",
      "[90]\tvalidation_0-rmse:3.13466\n",
      "[91]\tvalidation_0-rmse:3.13461\n",
      "[92]\tvalidation_0-rmse:3.13443\n",
      "[93]\tvalidation_0-rmse:3.13443\n",
      "[94]\tvalidation_0-rmse:3.13466\n",
      "[95]\tvalidation_0-rmse:3.13474\n",
      "[96]\tvalidation_0-rmse:3.13452\n",
      "[97]\tvalidation_0-rmse:3.13430\n",
      "[98]\tvalidation_0-rmse:3.13432\n",
      "[99]\tvalidation_0-rmse:3.13435\n",
      "[100]\tvalidation_0-rmse:3.13434\n",
      "[101]\tvalidation_0-rmse:3.13433\n",
      "[102]\tvalidation_0-rmse:3.13427\n",
      "[103]\tvalidation_0-rmse:3.13423\n",
      "[104]\tvalidation_0-rmse:3.13403\n",
      "[105]\tvalidation_0-rmse:3.13406\n",
      "[106]\tvalidation_0-rmse:3.13396\n",
      "[107]\tvalidation_0-rmse:3.13391\n",
      "[108]\tvalidation_0-rmse:3.13399\n",
      "[109]\tvalidation_0-rmse:3.13397\n",
      "[110]\tvalidation_0-rmse:3.13399\n",
      "[111]\tvalidation_0-rmse:3.13406\n",
      "[112]\tvalidation_0-rmse:3.13409\n",
      "[113]\tvalidation_0-rmse:3.13404\n",
      "[114]\tvalidation_0-rmse:3.13404\n",
      "[115]\tvalidation_0-rmse:3.13399\n",
      "[116]\tvalidation_0-rmse:3.13419\n",
      "[117]\tvalidation_0-rmse:3.13424\n",
      "[118]\tvalidation_0-rmse:3.13427\n",
      "[119]\tvalidation_0-rmse:3.13431\n",
      "[120]\tvalidation_0-rmse:3.13429\n",
      "[121]\tvalidation_0-rmse:3.13427\n",
      "[122]\tvalidation_0-rmse:3.13426\n",
      "[123]\tvalidation_0-rmse:3.13424\n",
      "[124]\tvalidation_0-rmse:3.13423\n",
      "[125]\tvalidation_0-rmse:3.13422\n",
      "[126]\tvalidation_0-rmse:3.13422\n",
      "[127]\tvalidation_0-rmse:3.13422\n",
      "[128]\tvalidation_0-rmse:3.13422\n",
      "[129]\tvalidation_0-rmse:3.13422\n",
      "[130]\tvalidation_0-rmse:3.13422\n",
      "[131]\tvalidation_0-rmse:3.13422\n",
      "[132]\tvalidation_0-rmse:3.13422\n",
      "[133]\tvalidation_0-rmse:3.13422\n",
      "[134]\tvalidation_0-rmse:3.13422\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3.125373597402936"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def grid_search(params, reg=XGBRegressor(missing=-999.0)):\n",
    "    grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=kfold)\n",
    "    grid_reg.fit(X_train_transformed, y_train)\n",
    "    best_params = grid_reg.best_params_\n",
    "    print(\"Best params:\", best_params)\n",
    "    best_score = (-grid_reg.best_score_) ** 0.5\n",
    "    print(\"Best score:\", best_score)\n",
    "\n",
    "grid_search(params={'max_depth':[1, 2, 3, 4, 6, 7, 8],\n",
    "                    'n_estimators':[31]})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best params: {'max_depth': 1, 'n_estimators': 31}\n",
      "Best score: 2.6632597695120817\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "grid_search(params={'max_depth':[1, 2, 3],\n",
    "                    'min_child_weight':[1,2,3,4,5],\n",
    "                    'n_estimators':[31]})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best params: {'max_depth': 1, 'min_child_weight': 1, 'n_estimators': 31}\n",
      "Best score: 2.6632597695120817\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "grid_search(params={'max_depth':[2],\n",
    "                    'min_child_weight':[2,3],\n",
    "                    'subsample':[0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "                    'n_estimators':[31, 50]})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best params: {'max_depth': 2, 'min_child_weight': 3, 'n_estimators': 31, 'subsample': 0.8}\n",
      "Best score: 2.680514895351618\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "grid_search(params={'max_depth':[1],\n",
    "                    'min_child_weight':[3],\n",
    "                    'subsample':[.8],\n",
    "                    'colsample_bytree':[0.9],\n",
    "                    'colsample_bylevel':[0.6, 0.7, 0.8, 0.9, 1],\n",
    "                    'colsample_bynode':[0.6, 0.7, 0.8, 0.9, 1],\n",
    "                    'n_estimators':[50]})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best params: {'colsample_bylevel': 0.7, 'colsample_bynode': 0.7, 'colsample_bytree': 0.9, 'max_depth': 1, 'min_child_weight': 3, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Best score: 2.6475633466646373\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "X_test_transformed = data_pipeline.fit_transform(X_test)\n",
    "model = XGBRegressor(max_depth=2, min_child_weight=3, subsample=0.9, colsample_bytree=0.8, gamma=2, missing=-999.0)\n",
    "model.fit(X_train_transformed, y_train)\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "rmse = MSE(y_pred, y_test)**0.5\n",
    "rmse\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.907554963248978"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "model = XGBRegressor(max_depth=1,\n",
    "                       min_child_weight=5,\n",
    "                       subsample=0.6,\n",
    "                       colsample_bytree=0.9,\n",
    "                       colsample_bylevel=0.9,\n",
    "                       colsample_bynode=0.8,\n",
    "                       n_estimators=50,\n",
    "                       missing=-999.0)\n",
    "\n",
    "model.fit(X_train_transformed, y_train)\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "rmse = MSE(y_pred, y_test)**0.5\n",
    "rmse"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.7911458213401037"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "full_pipeline = Pipeline([('null_imputer', NullValueImputer()),  ('sparse', SparseMatrix()),\n",
    "                          ('xgb', XGBRegressor(max_depth=1, min_child_weight=5, subsample=0.6,\n",
    "                                               colsample_bytree=0.9, colsample_bylevel=0.9, colsample_bynode=0.8,\n",
    "                                               missing=-999.0))])\n",
    "full_pipeline.fit(X, y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('null_imputer',\n",
       "                 <__main__.NullValueImputer object at 0x7fd7a6877af0>),\n",
       "                ('sparse', <__main__.SparseMatrix object at 0x7fd7a68775b0>),\n",
       "                ('xgb',\n",
       "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=0.9, colsample_bynode=0.8,\n",
       "                              colsample_bytree=0.9, gamma=0, gpu_id=-1,\n",
       "                              importance_type='gain',\n",
       "                              interaction_constraints='',\n",
       "                              learning_rate=0.300000012, max_delta_step=0,\n",
       "                              max_depth=1, min_child_weight=5, missing=-999.0,\n",
       "                              monotone_constraints='()', n_estimators=100,\n",
       "                              n_jobs=4, num_parallel_tree=1, random_state=0,\n",
       "                              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                              subsample=0.6, tree_method='exact',\n",
       "                              validate_parameters=1, verbosity=None))])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "new_data = X_test\n",
    "full_pipeline.predict(new_data)\n",
    "np.round(full_pipeline.predict(new_data))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([13.,  8., 11., 14., 13., 12., 14., 12., 11., 14., 13.,  9., 13.,\n",
       "       13., 14.,  8., 10., 10., 15., 10., 12., 13.,  7., 13.,  7.,  9.,\n",
       "       10., 13., 14., 12., 11., 12., 15.,  9., 11., 13., 12., 11.,  8.,\n",
       "       13., 11., 11., 12., 13., 13., 15., 13., 13., 13., 12., 14.,  6.,\n",
       "        6., 12., 15.,  9., 13.,  9., 14., 12., 13.,  7.,  9., 12., 14.,\n",
       "       11., 14., 14., 12., 11., 12., 13., 13.,  7., 14., 12., 13., 15.,\n",
       "       13., 10., 13.,  7., 11., 12., 13., 10.,  9., 13., 15., 15., 11.,\n",
       "       10., 14., 12., 13., 14., 12.,  8., 13., 15., 13.,  9., 12., 12.,\n",
       "       13., 14., 13., 10., 10., 14.,  7., 11., 13., 11., 14., 11., 12.,\n",
       "       11., 12., 12., 14.,  8., 13., 11., 14., 12., 15., 15., 12., 14.,\n",
       "       10., 14.,  9.,  9., 12., 13., 10., 12., 14., 13., 10., 13., 13.,\n",
       "       13., 13., 11., 12., 13., 14., 12.,  8., 10., 12.,  8.,  8., 13.,\n",
       "       14., 13., 13., 11., 12., 13.,  9.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "new_df = pd.read_csv('data/student-por.csv')\n",
    "new_X = df.iloc[:, :-3]\n",
    "new_y = df.iloc[:, -1]\n",
    "new_model = full_pipeline.fit(new_X, new_y)\n",
    "\n",
    "more_new_data = X_test[:25]\n",
    "np.round(new_model.predict(more_new_data))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([13.,  8., 11., 14., 13., 12., 14., 12., 11., 14., 13.,  9., 13.,\n",
       "       13., 14.,  8., 10., 10., 15., 10., 12., 13.,  7., 13.,  7.],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "single_row = X_test[:1]\n",
    "single_row_plus = pd.concat([single_row, X_test[:25]])\n",
    "print(np.round(new_model.predict(single_row_plus))[:1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[13.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from typing import Tuple\n",
    "import xgboost as xgb\n",
    "\n",
    "def f1_eval(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[str, float]:\n",
    "    y = dtrain.get_label()\n",
    "\n",
    "    # convert the predicted values from {predt E R | 0<predt<1} to {0, 1} with a threshold of 0.5\n",
    "    # all values less than 0.5 would be converted to 0 (False) and\n",
    "    # all values equal or greater than 0.5 would be converted to 1 (True)\n",
    "    predt_binary = np.where(predt > 0.5, 1, 0)\n",
    "    return \"F1_score\", f1_score(y_true=y, y_pred=predt_binary)\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_os_smote, label=y_train_os_smote)\n",
    "dtest = xgb.DMatrix(X_test_os_smote, label=y_test_os_smote)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # parameters for hypertunning \n",
    "    params = {\"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "              'n_estimators': trial.suggest_int('n_estimators',400, 600),\n",
    "              'max_depth': trial.suggest_int('max_depth', 10, 20),\n",
    "              \"eval_metric\": \"auc\",\n",
    "              'learning_rate': trial.suggest_uniform('learning_rate', 0.01, .1),\n",
    "              'subsample' : trial.suggest_uniform('subsample', 0.50, 1),\n",
    "              'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n",
    "              'gamma': trial.suggest_int('gamma', 0, 10),\n",
    "              \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 1.0),\n",
    "              \"sample_type\": trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"]),\n",
    "              'objective': 'binary:logistic',}\n",
    "              #'tree_method': 'gpu_hist',}\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-auc\")\n",
    "    best = xgb.train(params, dtrain, evals=[(dtest, \"validation\")], callbacks=[pruning_callback], feval=f1_eval)\n",
    "    best.save_model('xgb_zindi.model')\n",
    "    preds = np.rint(best.predict(dtest))\n",
    "    f1_score_ = f1_score(y_test_os_smote, preds)\n",
    "    return f1_score_\n",
    "\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective,n_trials=100)\n",
    "\n",
    "print(study.best_trial.params)\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "best_params['objective'] = 'binary:logistic'\n",
    "best_params['missing'] = -999\n",
    "best_params['random_state'] = 2020\n",
    "#best_params['tree_method'] = 'gpu_hist'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dtrain = lgb.Dataset(X_train_os_smote, label=y_train_os_smote)\n",
    "\n",
    "def f1_eval_(predt: np.ndarray, dtrain: lgb.Dataset) -> Tuple[str, float]:\n",
    "    y = dtrain.get_label()\n",
    "\n",
    "    # convert the predicted values from {predt E R | 0<predt<1} to {0, 1} with a threshold of 0.5\n",
    "    # all values less than 0.5 would be converted to 0 (False) and\n",
    "    # all values equal or greater than 0.5 would be converted to 1 (True)\n",
    "    predt_binary = np.where(predt > 0.5, 1, 0)\n",
    "    return \"F1_score\", f1_score(y_true=y, y_pred=predt_binary)\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"feature_pre_filter\": False,\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "    }\n",
    "\n",
    "    gbm = lgb.train(param, dtrain, feval=f1_eval_,)\n",
    "    preds = np.rint(gbm.predict(X_test_os_smote))\n",
    "    accuracy = accuracy_score(y_test_os_smote, preds,)\n",
    "    f1_score_ = f1_score(y_test_os_smote, preds)\n",
    "    f1_score(y_test_os_smote, preds)\n",
    "    return f1_score_\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_params_ = study.best_trial.params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fb32b4a763607e55b4eaa17ff711ea3be1e8d15fb59c66210835a4625460ab3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ml_3.9': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}